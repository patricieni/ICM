Automatically generated by Mendeley Desktop 1.17.11
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Bell2014,
abstract = {BACKGROUND Missing outcome data is a threat to the validity of treatment effect estimates in randomized controlled trials. We aimed to evaluate the extent, handling, and sensitivity analysis of missing data and intention-to-treat (ITT) analysis of randomized controlled trials (RCTs) in top tier medical journals, and compare our findings with previous reviews related to missing data and ITT in RCTs. METHODS Review of RCTs published between July and December 2013 in the BMJ, JAMA, Lancet, and New England Journal of Medicine, excluding cluster randomized trials and trials whose primary outcome was survival. RESULTS Of the 77 identified eligible articles, 73 (95%) reported some missing outcome data. The median percentage of participants with a missing outcome was 9% (range 0 - 70%). The most commonly used method to handle missing data in the primary analysis was complete case analysis (33, 45%), while 20 (27%) performed simple imputation, 15 (19%) used model based methods, and 6 (8%) used multiple imputation. 27 (35%) trials with missing data reported a sensitivity analysis. However, most did not alter the assumptions of missing data from the primary analysis. Reports of ITT or modified ITT were found in 52 (85%) trials, with 21 (40%) of them including all randomized participants. A comparison to a review of trials reported in 2001 showed that missing data rates and approaches are similar, but the use of the term ITT has increased, as has the report of sensitivity analysis. CONCLUSIONS Misqsing outcome data continues to be a common problem in RCTs. Definitions of the ITT approach remain inconsistent across trials. A large gap is apparent between statistical methods research related to missing data and use of these methods in application settings, including RCTs in top medical journals.},
author = {Bell, Melanie L and Fiero, Mallorie and Horton, Nicholas J and Hsu, Chiu-Hsieh},
doi = {10.1186/1471-2288-14-118},
file = {:Users/patricfulop/Library/Application Support/Mendeley Desktop/Downloaded/Bell et al. - 2014 - Handling missing data in RCTs a review of the top medical journals.pdf:pdf},
issn = {1471-2288},
journal = {BMC medical research methodology},
month = {nov},
pages = {118},
pmid = {25407057},
publisher = {BioMed Central},
title = {{Handling missing data in RCTs; a review of the top medical journals.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25407057 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4247714},
volume = {14},
year = {2014}
}
@article{Garcia-Laencina2015,
abstract = {Breast cancer is the most frequently diagnosed cancer in women. Using historical patient information stored in clinical datasets, data mining and machine learning approaches can be applied to predict the survival of breast cancer patients. A common drawback is the absence of information, i.e., missing data, in certain clinical trials. However, most standard prediction methods are not able to handle incomplete samples and, then, missing data imputation is a widely applied approach for solving this inconvenience. Therefore, and taking into account the characteristics of each breast cancer dataset, it is required to perform a detailed analysis to determine the most appropriate imputation and prediction methods in each clinical environment. This research work analyzes a real breast cancer dataset from Institute Portuguese of Oncology of Porto with a high percentage of unknown categorical information (most clinical data of the patients are incomplete), which is a challenge in terms of complexity. Four scenarios are evaluated: (I) 5-year survival prediction without imputation and 5-year survival prediction from cleaned dataset with (II) Mode imputation, (III) Expectation-Maximization imputation and (IV) K-Nearest Neighbors imputation. Prediction models for breast cancer survivability are constructed using four different methods: K-Nearest Neighbors, Classification Trees, Logistic Regression and Support Vector Machines. Experiments are performed in a nested ten-fold cross-validation procedure and, according to the obtained results, the best results are provided by the K-Nearest Neighbors algorithm: more than 81% of accuracy and more than 0.78 of area under the Receiver Operator Characteristic curve, which constitutes very good results in this complex scenario.},
annote = {You can write notes here!},
author = {Garc{\'{i}}a-Laencina, Pedro J. and Abreu, Pedro Henriques and Abreu, Miguel Henriques and Afonoso, No{\'{e}}mia},
doi = {10.1016/j.compbiomed.2015.02.006},
issn = {00104825},
journal = {Computers in Biology and Medicine},
keywords = {5-year survival prediction,Breast cancer,Discrete data,Imputation,Missing data},
month = {apr},
pages = {125--133},
pmid = {25725446},
title = {{Missing data imputation on the 5-year survival prediction of breast cancer patients with unknown discrete values}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25725446 http://linkinghub.elsevier.com/retrieve/pii/S0010482515000554},
volume = {59},
year = {2015}
}
@article{Little2012,
abstract = {Missing data in clinical trials can have a major effect on the validity of the inferences that can be drawn from the trial. This article reviews methods for preventing missing data and, failing that, dealing with data that are missing.},
author = {Little, Roderick J. and D'Agostino, Ralph and Cohen, Michael L. and Dickersin, Kay and Emerson, Scott S. and Farrar, John T. and Frangakis, Constantine and Hogan, Joseph W. and Molenberghs, Geert and Murphy, Susan A. and Neaton, James D. and Rotnitzky, Andrea and Scharfstein, Daniel and Shih, Weichung J. and Siegel, Jay P. and Stern, Hal},
doi = {10.1056/NEJMsr1203730},
file = {:Users/patricfulop/Library/Application Support/Mendeley Desktop/Downloaded/Little et al. - 2012 - The Prevention and Treatment of Missing Data in Clinical Trials.pdf:pdf},
issn = {0028-4793},
journal = {New England Journal of Medicine},
month = {oct},
number = {14},
pages = {1355--1360},
publisher = { Massachusetts Medical Society },
title = {{The Prevention and Treatment of Missing Data in Clinical Trials}},
url = {http://www.nejm.org/doi/abs/10.1056/NEJMsr1203730},
volume = {367},
year = {2012}
}
@article{Rubin1996,
abstract = {Multiple imputation was designed to handle the problem of missing data in public-use data bases where the data-base constructor and the ultimate user are distinct entities. The objective is valid frequency inference for ultimate users who in general have access only to complete-data software and possess limited knowledge of specific reasons and models for nonresponse. For this situation and objective, I believe that multiple imputation by the data-base constructor is the method of choice. This article first provides a description of the assumed context and objectives, and second, reviews the multiple imputation framework and its standard results. These preliminary discussions are especially important because some recent commentaries on multiple imputation have reflected either misunderstandings of the practical objectives of multiple imputation or misunderstandings of fundamental theoretical results. Then, criticisms of multiple imputation are considered, and, finally, comparisons are made to alternative strategies.},
author = {Rubin, Donald B.},
doi = {10.2307/2291635},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Data imputation,Datasets,Inference,Missing data,Modeling,Random allocation,Statistical estimation,Statistical models,Statistical variance,Statistics},
month = {jun},
number = {434},
pages = {473},
publisher = {Taylor & Francis, Ltd.American Statistical Association},
title = {{Multiple Imputation After 18+ Years}},
url = {http://www.jstor.org/stable/2291635?origin=crossref},
volume = {91},
year = {1996}
}
@article{Graham2007,
abstract = {Multiple imputation (MI) and full information maximum likelihood (FIML) are the two most common approaches to missing data analysis. In theory, MI and FIML are equivalent when identical models are tested using the same variables, and when m, the number of imputations performed with MI, approaches infinity. However, it is important to know how many imputations are necessary before MI and FIML are sufficiently equivalent in ways that are important to prevention scientists. MI theory suggests that small values of m, even on the order of three to five imputations, yield excellent results. Previous guidelines for sufficient m are based on relative efficiency, which involves the fraction of missing information (gamma) for the parameter being estimated, and m. In the present study, we used a Monte Carlo simulation to test MI models across several scenarios in which gamma and m were varied. Standard errors and p-values for the regression coefficient of interest varied as a function of m, but not at the same rate as relative efficiency. Most importantly, statistical power for small effect sizes diminished as m became smaller, and the rate of this power falloff was much greater than predicted by changes in relative efficiency. Based our findings, we recommend that researchers using MI should perform many more imputations than previously considered sufficient. These recommendations are based on gamma, and take into consideration one's tolerance for a preventable power falloff (compared to FIML) due to using too few imputations.},
author = {Graham, John W. and Olchowski, Allison E. and Gilreath, Tamika D.},
doi = {10.1007/s11121-007-0070-9},
issn = {1389-4986},
journal = {Prevention Science},
month = {aug},
number = {3},
pages = {206--213},
pmid = {17549635},
title = {{How Many Imputations are Really Needed? Some Practical Clarifications of Multiple Imputation Theory}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17549635 http://link.springer.com/10.1007/s11121-007-0070-9},
volume = {8},
year = {2007}
}
@article{Graham2009,
abstract = {This review presents a practical summary of the missing data literature, including a sketch of missing data theory and descriptions of normal-model multiple imputation (MI) and maximum likelihood methods. Practical missing data analysis issues are discussed, most notably the inclusion of auxiliary variables for improving power and reducing bias. Solutions are given for missing data challenges such as handling longitudinal, categorical, and clustered data with normal-model MI; including interactions in the missing data model; and handling large numbers of variables. The discussion of attrition and nonignorable missingness emphasizes the need for longitudinal diagnostics and for reducing the uncertainty about the missing data mechanism under attrition. Strategies suggested for reducing attrition bias include using auxiliary variables, collecting follow-up data on a sample of those initially missing, and collecting data on intent to drop out. Suggestions are given for moving forward with research on missing data and attrition.},
author = {Graham, John W.},
doi = {10.1146/annurev.psych.58.110405.085530},
file = {:Users/patricfulop/Library/Containers/com.apple.mail/Data/Library/Mail Downloads/D8D72B3D-2A40-4AF7-9172-F88930AB3BD3/Missing data analysis making it work in the real world Graham 2009.pdf:pdf},
isbn = {0066-4308 (Print)\n0066-4308 (Linking)},
issn = {0066-4308},
journal = {Annual Review of Psychology},
keywords = {attrition,maximum likelihood,missingness,multiple imputation,nonignorable,planned missingness},
number = {1},
pages = {549--576},
pmid = {18652544},
title = {{Missing Data Analysis: Making It Work in the Real World}},
url = {http://www.annualreviews.org/doi/10.1146/annurev.psych.58.110405.085530},
volume = {60},
year = {2009}
}
