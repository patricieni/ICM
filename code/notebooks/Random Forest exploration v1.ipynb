{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "import numpy as np\n",
    "import itertools  \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import time, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IMC_basedir = os.getcwd().split('code')[0]\n",
    "DATA_FILE = os.path.join(IMC_basedir, 'data/imputed_dataset_no_censoring_26022018_Amelia1.csv')\n",
    "DATA_MICE_FILE = os.path.join(IMC_basedir, 'data/imputed_dataset_no_censoring_26022018_MICE')\n",
    "TRAIN_FILE = os.path.join(IMC_basedir, 'data/amelia_train')\n",
    "TEST_FILE = os.path.join(IMC_basedir, 'data/amelia_test')\n",
    "MODEL_DIR = os.path.join(IMC_basedir, 'data/amelia_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binning(col, cut_points, labels=None):\n",
    "    #Define min and max values:\n",
    "    minval = col.min()\n",
    "    maxval = col.max()\n",
    "\n",
    "    #create list by adding min and max to cut_points\n",
    "    break_points = [minval] + cut_points + [maxval]\n",
    "\n",
    "    # if no labels provided, use default labels 0 ... (n-1)\n",
    "    if not labels:\n",
    "        labels = range(len(cut_points)+1)\n",
    "\n",
    "    #Binning using cut function of pandas\n",
    "    colBin = pd.cut(col,bins=break_points,labels=labels,include_lowest=True)\n",
    "    return colBin\n",
    "\n",
    "def plot_report(Y_true, Y_pred):\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(confusion_matrix(Y_test, Y_pred), \n",
    "                          classes=labels, title='Confusion matrix')\n",
    "\n",
    "    print(classification_report(Y_test, Y_pred, target_names=labels))\n",
    "    print('Accuracy: {}'.format(accuracy_score(Y_test, Y_pred)))\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "def plot_hist(history, save_as=None):\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    if save_as:\n",
    "        plt.savefig(save_as + '_acc.jpg')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    if save_as:\n",
    "        plt.savefig(save_as + '_loss.jpg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_mice = pd.read_csv(DATA_MICE_FILE)\n",
    "df_mice.drop(\"Unnamed: 0\", axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df_amelia = pd.read_csv(DATA_FILE)\n",
    "df_amelia.drop(\"Unnamed: 0\", axis = 1, inplace=True)\n",
    "labels = [\"1.5year\",\"4years\",\"more\"]\n",
    "cut_points = [500,1500]\n",
    "\n",
    "#labels = [\"3_months\",\"6_months\",\"9_months\",\"12_months\",\"15_months\",\"18_months\",\"2_years\",\"3_years\",\"4_years\",\"5_years\",\"10_years\",\"10_plus_years\"]\n",
    "#cut_points = [90,180,270,360,450,540,720,1095,1460,1825,3650]\n",
    "df_amelia.loc[:,\"life_expectancy_bin\"] = binning(df_amelia.life_expectancy, cut_points, labels)\n",
    "\n",
    "df_amelia['life_expectancy_bin'] = LabelEncoder().fit_transform(df_amelia['life_expectancy_bin'])\n",
    "#df_amelia.drop(\"life_expectancy\", axis=1, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "le_dict = dict() # Initialise an empty dictionary to keep all LabelEncoders\n",
    "df_categories = df_amelia.copy(deep=True) \n",
    "# Loop over attributes by excluding the continuous oness\n",
    "for column in df_categories.drop(['Age_surgery', 'life_expectancy', 'Tumor_grade','IDH_TERT','IK'], axis=1):  \n",
    "    le = LabelEncoder().fit(df_categories[column]) # Initialise the LabelEncoder and fit\n",
    "    df_categories[column] = le.transform(df_categories[column]) # Transform data and save in credit_clean DataFrame\n",
    "    le_dict[column] = le # Store the LabelEncdoer in dictionary\n",
    "    \n",
    "df = df_amelia.copy(deep=True)\n",
    "non_dummy_cols = ['Tumor_grade','IDH_TERT','life_expectancy','life_expectancy_bin','Gender','IK','Age_surgery']\n",
    "dummy_cols = list(set(df.columns) - set(non_dummy_cols))\n",
    "\n",
    "df = pd.get_dummies(df,columns=dummy_cols)\n",
    "\n",
    "df.Gender.replace(to_replace={'M':1, 'F':0},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_train_test_data(data_df, train_size=0.8):\n",
    "    df_tensorflow = data_df.copy(deep=True)\n",
    "    X_train, X_test = train_test_split(df_tensorflow, train_size=train_size, \n",
    "                                       test_size=1-train_size)\n",
    "\n",
    "    # remove columns\n",
    "    X_train.drop('life_expectancy', axis = 1, inplace=True)\n",
    "    X_test.drop('life_expectancy', axis = 1, inplace=True)\n",
    "\n",
    "    Y_train = X_train['life_expectancy_bin']\n",
    "    X_train.drop('life_expectancy_bin', axis = 1, inplace=True)\n",
    "\n",
    "    Y_test = X_test['life_expectancy_bin']\n",
    "    X_test.drop('life_expectancy_bin', axis = 1, inplace=True)\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Classification: Benchmark Logistic Regression (Scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = get_train_test_data(df, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'bootstrap': True,\n",
      " 'criterion': 'mse',\n",
      " 'max_depth': None,\n",
      " 'max_features': 'auto',\n",
      " 'max_leaf_nodes': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_impurity_split': None,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 10,\n",
      " 'n_jobs': 1,\n",
      " 'oob_score': False,\n",
      " 'random_state': 182,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(random_state = 182)\n",
    "\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(rf.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': [True, False],\n",
      " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
      " 'max_features': ['auto', 'sqrt'],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'min_samples_split': [2, 5, 10],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor(random_state = 42)\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid,\n",
    "                              n_iter = 5, scoring='neg_mean_absolute_error', \n",
    "                              cv = 3, verbose=2, random_state=42, n_jobs=-1,\n",
    "                              return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "[CV] n_estimators=200, min_samples_split=10, min_samples_leaf=2, max_depth=50, max_features=sqrt, bootstrap=True \n",
      "[CV] n_estimators=200, min_samples_split=10, min_samples_leaf=2, max_depth=50, max_features=sqrt, bootstrap=True \n",
      "[CV] n_estimators=200, min_samples_split=10, min_samples_leaf=2, max_depth=50, max_features=sqrt, bootstrap=True \n",
      "[CV] n_estimators=600, min_samples_split=10, min_samples_leaf=4, max_depth=90, max_features=sqrt, bootstrap=False \n",
      "[CV] n_estimators=600, min_samples_split=10, min_samples_leaf=4, max_depth=90, max_features=sqrt, bootstrap=False \n",
      "[CV] n_estimators=600, min_samples_split=10, min_samples_leaf=4, max_depth=90, max_features=sqrt, bootstrap=False \n",
      "[CV] n_estimators=600, min_samples_split=2, min_samples_leaf=2, max_depth=60, max_features=auto, bootstrap=False \n",
      "[CV] n_estimators=600, min_samples_split=2, min_samples_leaf=2, max_depth=60, max_features=auto, bootstrap=False \n",
      "[CV]  n_estimators=200, min_samples_split=10, min_samples_leaf=2, max_depth=50, max_features=sqrt, bootstrap=True, total=   0.6s\n",
      "[CV] n_estimators=600, min_samples_split=2, min_samples_leaf=2, max_depth=60, max_features=auto, bootstrap=False \n",
      "[CV]  n_estimators=200, min_samples_split=10, min_samples_leaf=2, max_depth=50, max_features=sqrt, bootstrap=True, total=   0.6s\n",
      "[CV]  n_estimators=200, min_samples_split=10, min_samples_leaf=2, max_depth=50, max_features=sqrt, bootstrap=True, total=   0.6s\n",
      "[CV] n_estimators=1400, min_samples_split=5, min_samples_leaf=1, max_depth=30, max_features=sqrt, bootstrap=True \n",
      "[CV] n_estimators=1400, min_samples_split=5, min_samples_leaf=1, max_depth=30, max_features=sqrt, bootstrap=True \n",
      "[CV]  n_estimators=600, min_samples_split=10, min_samples_leaf=4, max_depth=90, max_features=sqrt, bootstrap=False, total=   1.9s\n",
      "[CV] n_estimators=1400, min_samples_split=5, min_samples_leaf=1, max_depth=30, max_features=sqrt, bootstrap=True \n",
      "[CV]  n_estimators=600, min_samples_split=10, min_samples_leaf=4, max_depth=90, max_features=sqrt, bootstrap=False, total=   1.9s\n",
      "[CV] n_estimators=1000, min_samples_split=10, min_samples_leaf=1, max_depth=80, max_features=auto, bootstrap=False \n",
      "[CV]  n_estimators=600, min_samples_split=10, min_samples_leaf=4, max_depth=90, max_features=sqrt, bootstrap=False, total=   1.9s\n",
      "[CV] n_estimators=1000, min_samples_split=10, min_samples_leaf=1, max_depth=80, max_features=auto, bootstrap=False \n",
      "[CV]  n_estimators=1400, min_samples_split=5, min_samples_leaf=1, max_depth=30, max_features=sqrt, bootstrap=True, total=   4.4s\n",
      "[CV] n_estimators=1000, min_samples_split=10, min_samples_leaf=1, max_depth=80, max_features=auto, bootstrap=False \n",
      "[CV]  n_estimators=1400, min_samples_split=5, min_samples_leaf=1, max_depth=30, max_features=sqrt, bootstrap=True, total=   4.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   8 out of  15 | elapsed:    5.4s remaining:    4.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=600, min_samples_split=2, min_samples_leaf=2, max_depth=60, max_features=auto, bootstrap=False, total=   6.2s\n",
      "[CV]  n_estimators=600, min_samples_split=2, min_samples_leaf=2, max_depth=60, max_features=auto, bootstrap=False, total=   6.2s\n",
      "[CV]  n_estimators=1400, min_samples_split=5, min_samples_leaf=1, max_depth=30, max_features=sqrt, bootstrap=True, total=   4.3s\n",
      "[CV]  n_estimators=600, min_samples_split=2, min_samples_leaf=2, max_depth=60, max_features=auto, bootstrap=False, total=   6.1s\n",
      "[CV]  n_estimators=1000, min_samples_split=10, min_samples_leaf=1, max_depth=80, max_features=auto, bootstrap=False, total=   7.7s\n",
      "[CV]  n_estimators=1000, min_samples_split=10, min_samples_leaf=1, max_depth=80, max_features=auto, bootstrap=False, total=   7.7s\n",
      "[CV]  n_estimators=1000, min_samples_split=10, min_samples_leaf=1, max_depth=80, max_features=auto, bootstrap=False, total=   6.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:   11.9s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise',\n",
       "          estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid=True, n_iter=5, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_leaf': [1, 2, 4], 'max_features': ['auto', 'sqrt'], 'bootstrap': [True, False], 'min_samples_split': [2, 5, 10]},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score=True, scoring='neg_mean_absolute_error',\n",
       "          verbose=2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the random search model\n",
    "rf_random.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'max_depth': 30,\n",
       " 'max_features': 'sqrt',\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 5,\n",
       " 'n_estimators': 1400}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.57640791, 1.81294592, 6.09864934, 4.13554637, 7.15006399]),\n",
       " 'mean_score_time': array([0.02936673, 0.08575805, 0.07761224, 0.21870097, 0.09134579]),\n",
       " 'mean_test_score': array([-0.50686467, -0.50366319, -0.54021666, -0.50328853, -0.52879843]),\n",
       " 'mean_train_score': array([-0.39001478, -0.36701985, -0.12369554, -0.29812566, -0.21711964]),\n",
       " 'param_bootstrap': masked_array(data=[True, False, False, True, False],\n",
       "              mask=[False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[50, 90, 60, 30, 80],\n",
       "              mask=[False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_features': masked_array(data=['sqrt', 'sqrt', 'auto', 'sqrt', 'auto'],\n",
       "              mask=[False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_samples_leaf': masked_array(data=[2, 4, 2, 1, 1],\n",
       "              mask=[False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_samples_split': masked_array(data=[10, 10, 2, 5, 10],\n",
       "              mask=[False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[200, 600, 600, 1400, 1000],\n",
       "              mask=[False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'bootstrap': True,\n",
       "   'max_depth': 50,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 10,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'max_depth': 90,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 4,\n",
       "   'min_samples_split': 10,\n",
       "   'n_estimators': 600},\n",
       "  {'bootstrap': False,\n",
       "   'max_depth': 60,\n",
       "   'max_features': 'auto',\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 600},\n",
       "  {'bootstrap': True,\n",
       "   'max_depth': 30,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 1400},\n",
       "  {'bootstrap': False,\n",
       "   'max_depth': 80,\n",
       "   'max_features': 'auto',\n",
       "   'min_samples_leaf': 1,\n",
       "   'min_samples_split': 10,\n",
       "   'n_estimators': 1000}],\n",
       " 'rank_test_score': array([3, 2, 5, 1, 4], dtype=int32),\n",
       " 'split0_test_score': array([-0.51958509, -0.51486822, -0.54156575, -0.51332694, -0.53905571]),\n",
       " 'split0_train_score': array([-0.38247745, -0.36033402, -0.12517586, -0.29190576, -0.2077131 ]),\n",
       " 'split1_test_score': array([-0.5046668 , -0.50286617, -0.5577493 , -0.50365   , -0.54416786]),\n",
       " 'split1_train_score': array([-0.38494385, -0.3614587 , -0.11995454, -0.29382767, -0.22602361]),\n",
       " 'split2_test_score': array([-0.49634212, -0.49325516, -0.52133493, -0.49288866, -0.50317171]),\n",
       " 'split2_train_score': array([-0.40262303, -0.37926683, -0.12595624, -0.30864354, -0.21762223]),\n",
       " 'std_fit_time': array([0.00900174, 0.01768969, 0.0505119 , 0.03955247, 0.60020268]),\n",
       " 'std_score_time': array([0.00034408, 0.00252021, 0.01417794, 0.01226813, 0.00254527]),\n",
       " 'std_test_score': array([0.00961533, 0.00884147, 0.01489668, 0.00834781, 0.01824061]),\n",
       " 'std_train_score': array([0.00897206, 0.00867209, 0.00266441, 0.00747854, 0.00748368])}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    errors = abs(predictions - test_labels)\n",
    "    mape = 100 * np.mean(errors / test_labels)\n",
    "    accuracy = 100 - mape\n",
    "    print('Model Performance')\n",
    "    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Average Error: 0.4770 degrees.\n",
      "Accuracy = -inf%.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate baseline\n",
    "base_model = RandomForestRegressor(n_estimators = 10, random_state = 42)\n",
    "base_model.fit(X_train, Y_train)\n",
    "base_accuracy = evaluate(base_model, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Average Error: 0.4713 degrees.\n",
      "Accuracy = -inf%.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the Best Random Search Model\n",
    "best_random = rf_random.best_estimator_\n",
    "random_accuracy = evaluate(best_random, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improvement of nan%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paul.pop/miniconda3/envs/KS3/lib/python3.5/site-packages/ipykernel/__main__.py:1: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "rf = RandomForestRegressor(random_state = 42)\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, Y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "grid_accuracy = evaluate(best_grid, test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def larger_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, kernel_initializer=\"normal\", input_dim=54, activation=\"relu\"))\n",
    "    model.add(Dense(6, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test_kfolds(KerasRegressor(build_fn=larger_model, nb_epoch=50, batch_size=5, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wider_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=54, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_kfolds(KerasRegressor(build_fn=wider_model, nb_epoch=50, batch_size=5, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mlp_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50, input_dim = 54, kernel_initializer='he_normal', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, kernel_initializer='he_normal'))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    adam = optimizers.Adam(lr = 0.001)\n",
    "    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# create 5 models to ensemble\n",
    "model1 = KerasClassifier(build_fn = mlp_model, epochs = 100)\n",
    "model2 = KerasClassifier(build_fn = mlp_model, epochs = 100)\n",
    "model3 = KerasClassifier(build_fn = mlp_model, epochs = 100)\n",
    "model4 = KerasClassifier(build_fn = mlp_model, epochs = 100)\n",
    "model5 = KerasClassifier(build_fn = mlp_model, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ensemble_clf = VotingClassifier(estimators = [('model1', model1), ('model2', model2), ('model3', model3), ('model4', model4), ('model5', model5)], voting = 'soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ensemble_clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = get_train_test_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, kernel_initializer=\"normal\", input_dim=54))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "    \n",
    "model.compile(optimizer = optimizers.SGD(lr = 0.001), loss = 'mean_squared_error', metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(np.array(X_train), np.array(Y_train), epochs = 100, verbose = 1)\n",
    "\n",
    "# plt.plot(history.history['acc'])\n",
    "# plt.plot(history.history['val_acc'])\n",
    "# plt.legend(['training', 'validation'], loc = 'upper left')\n",
    "# plt.show()\n",
    "\n",
    "results = model.evaluate(X_test, Y_test)\n",
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Categorical predictions\n",
    "from keras.utils.np_utils import to_categorical\n",
    "X = df.copy(deep=True)\n",
    "\n",
    "Y = np.array(to_categorical(X['life_expectancy_bin']))\n",
    "# remove columns\n",
    "X.drop('life_expectancy', axis = 1, inplace=True)\n",
    "X.drop('life_expectancy_bin', axis = 1, inplace=True)\n",
    "\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "model = Sequential()\n",
    "# model.add(Dense(108, input_shape=(54,), kernel_initializer='normal', activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(32, kernel_initializer='normal', activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(16, kernel_initializer='normal', activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(8, kernel_initializer='normal', activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.add(Dense(64, activation='relu', input_dim=54))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# model.add(Dense(input_dim=54, output_dim=12, activation='relu'))\n",
    "# model.add(Dropout(0.1))\n",
    "# model.add(Dense(input_dim=12, output_dim=12, activation='relu'))\n",
    "# model.add(Dropout(0.1))\n",
    "# model.add(Dense(output_dim=1, activation='softmax'))\n",
    "# model.compile(loss='mean_squared_error', optimizer='adadelta', metrics=['accuracy'])\n",
    "\n",
    "# 3\n",
    "#tbCallBack = keras.callbacks.TensorBoard(log_dir='/tmp/keras_logs', write_graph=True)\n",
    "\n",
    "# 4\n",
    "#model.compile(loss='mean_squared_error', optimizer='adadelta', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "hist = model.fit(X, Y, epochs=600, batch_size=128,  verbose=1, validation_split=0.4)#, callbacks=[tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_hist(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_hist(hist) #, save_as='deep_5_wide_108')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scikit-Neural Network (SKNN)\n",
    "\n",
    "Different approach for Classification vs. Regression using Neural Network:\n",
    "- Training examples: Rn x {class_1, ..., class_n} (one-hot encoding) vs Rn x Rm\n",
    "- Last layer: softmax vs linear / sigmoid\n",
    "- Loss function: Cross entropy vs MSE / Absolute error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. SKNN: Regression (sine function example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, Y = make_regression(n_features=54, n_informative=2,random_state=0, shuffle=False)\n",
    "\n",
    "regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "regr.fit(X_train, Y_train)\n",
    "\n",
    "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,\n",
    "           max_features='auto', max_leaf_nodes=None,\n",
    "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "           min_samples_leaf=1, min_samples_split=2,\n",
    "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
    "           oob_score=False, random_state=0, verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(regr.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'downsample'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-59f5ca4ed5ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Fitting the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mnn01\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msine_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msine_y_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Making the prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/paul.pop/miniconda3/envs/KS3/lib/python3.5/site-packages/sknn/mlp.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, w)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRegressor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/paul.pop/miniconda3/envs/KS3/lib/python3.5/site-packages/sknn/mlp.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, w)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training on dataset of {:,} samples with {} total size.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/paul.pop/miniconda3/envs/KS3/lib/python3.5/site-packages/sknn/mlp.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, X, y, w)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_specs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiLayerPerceptronBackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/paul.pop/miniconda3/envs/KS3/lib/python3.5/site-packages/sknn/backend/__init__.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"No backend for module sknn was imported.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/paul.pop/miniconda3/envs/KS3/lib/python3.5/site-packages/sknn/backend/lasagne/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultiLayerPerceptronBackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Register this implementation as the MLP backend.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/paul.pop/miniconda3/envs/KS3/lib/python3.5/site-packages/sknn/backend/lasagne/mlp.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonlinearities\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/paul.pop/miniconda3/envs/KS3/lib/python3.5/site-packages/lasagne/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnonlinearities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mobjectives\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/paul.pop/miniconda3/envs/KS3/lib/python3.5/site-packages/lasagne/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/paul.pop/miniconda3/envs/KS3/lib/python3.5/site-packages/lasagne/layers/pool.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mas_tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownsample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'downsample'"
     ]
    }
   ],
   "source": [
    "from sknn.mlp import Regressor, Layer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Build the dataset\n",
    "sine_X = np.linspace(0, 2 * 2 * math.pi, 1001)  \n",
    "sine_y = 5 * np.sin(sine_X)\n",
    "sine_X_train, sine_X_test, sine_y_train, sine_y_test = train_test_split(sine_X, sine_y, test_size=0.33, random_state=42)\n",
    "# Transform 1D arrays to 2D arrays\n",
    "sine_X_train = sine_X_train[:,None]\n",
    "sine_X_test = sine_X_test[:,None]\n",
    "sine_y_train = sine_y_train[:,None]\n",
    "sine_y_test = sine_y_test[:,None]\n",
    "\n",
    "# Build the NN\n",
    "nn01 = Regressor(layers=[# input layer is added automatically, based on the number of features\n",
    "                        # a bias unit is added automatically for every input and hiddens layers\n",
    "                        Layer(\"Tanh\", units=30),      # 1st hidden layer: Rectifier, Sigmoid or Tanh\n",
    "                        Layer(\"Tanh\", units=30),      # 2nd hidden layer: Rectifier, Sigmoid or Tanh\n",
    "                        Layer(\"Linear\", units=1)],    # output layer                           \n",
    "                learning_rate=0.001,\n",
    "                weight_decay=0.1,                     # weight_decay = regularization\n",
    "                regularize='L2',\n",
    "                learning_momentum=0.66,\n",
    "                n_iter=50,\n",
    "                batch_size=1,                         # batch_size=1: each sample is treated on its own\n",
    "                loss_type='mse',\n",
    "                verbose=True)\n",
    "\n",
    "# Fitting the model\n",
    "nn01.fit(sine_X_train, sine_y_train)\n",
    "\n",
    "# Making the prediction\n",
    "sine_y_pred = nn01.predict(sine_X_test) \n",
    "\n",
    "# Results\n",
    "print(\"Results of SKNN Regression:\")\n",
    "print(\"Residual sum of squares (MSE): %.2f\" % np.mean((sine_y_pred - sine_y_test) ** 2))\n",
    "print('Variance score: %.2f' % nn01.score(sine_X_test, sine_y_test)) # Explained variance score: 1 is perfect prediction\n",
    "\n",
    "# Plot outputs\n",
    "plt.clf() \n",
    "plt.scatter(sine_X_train, sine_y_train,  color='black', label='training data', alpha=0.1)\n",
    "plt.scatter(sine_X_test, sine_y_test,  color='red', label='test data')\n",
    "plt.scatter(sine_X_test, sine_y_pred, color='blue', label='NN prediction') ## NN\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomized Search for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Specify parameters and distributions to sample from\n",
    "param_dist = {  \"learning_rate\"     : np.logspace(-4, -2, 3),  # 3 numbers from 1e-4 to 1e-2\n",
    "                \"weight_decay\"      : np.logspace(-4, -2, 3),\n",
    "                \"learning_momentum\" : [0.33, 0.66, 0.99],\n",
    "                \"n_iter\"            : [30, 40, 50]}\n",
    "\n",
    "# Start the Randomized Search\n",
    "n_iter_search = 20\n",
    "random_search = RandomizedSearchCV(nn01, param_distributions=param_dist, n_iter=n_iter_search, \n",
    "                                   scoring='mean_squared_error', n_jobs=-1, cv=3, verbose=3)\n",
    "\n",
    "random_search = random_search.fit(sine_X_train, sine_y_train)\n",
    "print \"Best parameters set found on development set:\"\n",
    "print random_search.best_score_, random_search.best_params_\n",
    "print\n",
    "print \"Grid scores on development set:\"\n",
    "for params, mean_score, scores in random_search.grid_scores_:\n",
    "    print \"%0.3f (+/-%0.03f) for %r\" % (mean_score, scores.std() * 2, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn01_opt = Regressor(layers=[# input layer is added automatically, based on the number of features\n",
    "                             # a bias unit is added automatically for every input and hiddens layers\n",
    "                             Layer(\"Tanh\", units=30),      # 1st hidden layer: Rectifier, Sigmoid or Tanh\n",
    "                             Layer(\"Tanh\", units=30),      # 2nd hidden layer: Rectifier, Sigmoid or Tanh\n",
    "                             Layer(\"Linear\", units=1)],    # output layer                           \n",
    "                learning_rate=random_search.best_params_['learning_rate'],  \n",
    "                weight_decay=random_search.best_params_['weight_decay'],      \n",
    "                regularize='L2',\n",
    "                learning_momentum=random_search.best_params_['learning_momentum'],\n",
    "                n_iter=random_search.best_params_['n_iter'],\n",
    "                batch_size=1,    # batch_size=1: each sample is treated on its own\n",
    "                loss_type='mse',\n",
    "                verbose=True)\n",
    "\n",
    "# Fitting the model\n",
    "nn01_opt.fit(sine_X_train, sine_y_train)\n",
    "\n",
    "# Plot outputs\n",
    "plt.clf() \n",
    "plt.scatter(sine_X_train, sine_y_train,  color='black', label='training data', alpha=0.1)\n",
    "plt.scatter(sine_X_test, sine_y_test,  color='red', label='test data')\n",
    "plt.scatter(sine_X_test, nn01_opt.predict(sine_X_test) , color='blue', label='NN prediction') ## NN\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print \"Residual sum of squares (MSE): %.2f\" % np.mean((nn01_opt.predict(sine_X_test) - sine_y_test) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title = 'Learning Curves (NN with tuned hyperparameters)'    \n",
    "estimator = nn01_opt # regressor with tuned hyperparameters \n",
    "plot_learning_curve(estimator, title, sine_X_train, sine_y_train, \n",
    "                    ylim=(-10., 0.), cv=5, n_jobs=-1, scoring='mean_squared_error')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: It outputs the negative of the MSE, as it always tries to maximize the score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pybrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pip install git+https://github.com/pybrain/pybrain.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 PyBrain: Regression (diabetes dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pybrain2\n",
    "from pybrain.structure import SigmoidLayer, LinearLayer, TanhLayer\n",
    "from pybrain.datasets import SupervisedDataSet\n",
    "from pybrain.supervised.trainers import BackpropTrainer\n",
    "import pybrain.tools.shortcuts as pb\n",
    "import numpy, math\n",
    "\n",
    "# Build the dataset\n",
    "xvalues = diabetes_X_train[:,0]   #should be normalized when lots of features\n",
    "yvalues = diabetes_y_train\n",
    "ds = SupervisedDataSet(1, 1)\n",
    "for x, y in zip(xvalues, yvalues):\n",
    "    ds.addSample((x), (y))\n",
    "    \n",
    "# Build the NN\n",
    "nn1 = pb.buildNetwork(1,  # 1 input node\n",
    "                   20,    # number of nodes in 1st hidden layer\n",
    "                   #10,   # number of nodes in 2nd hidden layer\n",
    "                   1,     # 1 output node\n",
    "                   bias = False,\n",
    "                   hiddenclass = SigmoidLayer,\n",
    "                   outclass = LinearLayer )\n",
    "\n",
    "# Train the NN\n",
    "trainer = BackpropTrainer(nn1, ds, learningrate = 0.01, weightdecay=0.01, momentum=0.6) #, verbose = True)\n",
    "train_mse, validation_mse = trainer.trainUntilConvergence(maxEpochs = 20, continueEpochs=5, validationProportion=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note on some of the parameters\n",
    "\n",
    "**validationProportion**: ratio of the dataset that is used for the validation dataset.\n",
    "If maxEpochs is given, at most that many epochs are trained. Each time validation error hits a minimum, try for continueEpochs # epochs to find a better one.\n",
    "\n",
    "**Epoch**: one epoch means that every example has been seen once. It is preferable to track epochs rather than iterations since \n",
    "the number of iterations depends on the arbitrary setting of batch size. Batchs are used for example in the minibatch method,\n",
    "for example, for 1000 examples, the NN is trained on examples 1-100, then examples 101-201, etc.\n",
    "\n",
    "**Momentum**: 0 < m < 1 is a global parameter which must be determined by trial and error. Momentum simply adds a fraction m of the previous weight update to the current one. When the gradient keeps pointing in the same direction, this will increase the size of the steps taken towards the minimum. It is otherefore often necessary to reduce the global learning rate µ when using a lot of momentum (m close to 1). If you combine a high learning rate with a lot of momentum, you will rush past the minimum with huge steps! When the gradient keeps changing direction, momentum will smooth out the variations. Adding a momentum can help to speed up convergence to the minimum by damping oscillations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make the prediction\n",
    "y_pred = [ nn1.activate([x]) for x in diabetes_X_test ]  \n",
    "\n",
    "# Print the weights\n",
    "def weight_connection(n):\n",
    "    for mod in n.modules:\n",
    "        for conn in n.connections[mod]:\n",
    "            print conn\n",
    "            for cc in range(len(conn.params)):\n",
    "                print conn.whichBuffers(cc), conn.params[cc]\n",
    "#weight_connection(nn1)\n",
    "\n",
    "# And evaluate:\n",
    "\n",
    "# Fitting error\n",
    "plt.clf() \n",
    "plt.scatter(diabetes_X_train, diabetes_y_train,  color='black', alpha=0.1)\n",
    "plt.scatter(diabetes_X_test, diabetes_y_test,  color='red', label = 'target')\n",
    "plt.scatter(diabetes_X_test, y_pred, color = 'blue', label = 'NN output')  # NN\n",
    "plt.scatter(diabetes_X_test, diabetes_y_pred_lin, color='green', label='linear model')  # benchmark\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Learning curves\n",
    "plt.clf()\n",
    "plt.plot(range(len(train_mse)), np.sqrt(train_mse), color='blue', label='training error')\n",
    "plt.plot(range(len(validation_mse)), np.sqrt(validation_mse), color='red', label='validation error')\n",
    "plt.title('Learning curves: loss(=RMSE) as a function of Epochs')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 PyBrain: Regression (diabetes dataset)\n",
    "\n",
    "Similar as above (section 3.1), except here we are creating the feeforward neural network from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pybrain.structure import FeedForwardNetwork, LinearLayer, SigmoidLayer, TanhLayer, BiasUnit, FullConnection\n",
    "from pybrain.supervised.trainers import BackpropTrainer\n",
    "\n",
    "nn2 = FeedForwardNetwork()\n",
    "\n",
    "bias = False\n",
    "\n",
    "\n",
    "## Constructing the input, hidden and output layers:\n",
    "\n",
    "# In order to use them, we have to add them to the network:\n",
    "nn2.addInputModule( LinearLayer(1, name = 'in') )      # inLayer\n",
    "nn2.addModule( SigmoidLayer(5, name = 'hidden0') )     # hiddenLayer\n",
    "nn2.addOutputModule( LinearLayer(1, name = 'out') )    # outLayer\n",
    "if bias:\n",
    "    nn2.addInputModule( BiasUnit(name = 'inbias') )        # bias for input layer\n",
    "    nn2.addModule( BiasUnit(name = 'bias0') )              # bias for hidden layer\n",
    "\n",
    "# As with modules, we have to explicitly add them to the network:\n",
    "theta1 = FullConnection(nn2['in'], nn2['hidden0'])\n",
    "theta2 = FullConnection(nn2['hidden0'], nn2['out'])\n",
    "nn2.addConnection( theta1 )    # in_to_hidden connections\n",
    "nn2.addConnection( theta2 )    # hidden_to_out connections\n",
    "if bias:\n",
    "    nn2.addConnection(FullConnection(nn2['inbias'], nn2['hidden0']))\n",
    "    nn2.addConnection(FullConnection(nn2['bias0'], nn2['out']))\n",
    "\n",
    "nn2.sortModules()     # making the MLP usable\n",
    "\n",
    "\n",
    "# Build the dataset ***with bias units***\n",
    "xvalues = diabetes_X_train[:,0]   # Should be normalized when lots of features\n",
    "yvalues = diabetes_y_train\n",
    "ds2 = SupervisedDataSet(1, 1)     # Dataset for Supervised Regression Training\n",
    "# No need to add here the bias term to the feature matrix: it will be added in the training method\n",
    "for x, y in zip(xvalues, yvalues):\n",
    "    ds2.addSample((x), (y))       # ds.addSample((x1, ...., xn), (y)) # for each training example\n",
    "\n",
    "# shows the nn2 weights\n",
    "weight_connection(nn2)\n",
    "    \n",
    "# Train the NN\n",
    "trainer = BackpropTrainer(nn2, ds2, learningrate = 0.01, weightdecay=0.01, momentum=0.0) #, verbose = True)\n",
    "train_mse, validation_mse = trainer.trainUntilConvergence(maxEpochs = 20, continueEpochs=5, validationProportion=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make the predictions\n",
    "y_pred = [ nn2.activate([x]) for x in diabetes_X_test ]  \n",
    "\n",
    "# Evaluate: \n",
    "\n",
    "# Plot outputs\n",
    "plt.clf() \n",
    "plt.scatter(diabetes_X_train, diabetes_y_train,  color='black', alpha=0.1)\n",
    "plt.scatter(diabetes_X_test, diabetes_y_test,  color='red', label = 'target')\n",
    "plt.scatter(diabetes_X_test, y_pred, color = 'blue', label = 'NN output')\n",
    "plt.scatter(diabetes_X_test, diabetes_y_pred_lin, color='green', label='linear model')  # benchmark\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Learning curves\n",
    "plt.clf()\n",
    "plt.plot(range(len(train_mse)), np.sqrt(train_mse), color='blue', label='training error')\n",
    "plt.plot(range(len(validation_mse)), np.sqrt(validation_mse), color='red', label='validation error')\n",
    "plt.title('Learning curves: loss(=RMSE) as a function of Epochs')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 PyBrain: Regression (sine function example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pybrain.structure import SigmoidLayer, LinearLayer, TanhLayer\n",
    "from pybrain.tools.shortcuts import buildNetwork\n",
    "from pybrain.datasets import SupervisedDataSet\n",
    "from pybrain.supervised.trainers import BackpropTrainer\n",
    "\n",
    "# Build the dataset\n",
    "xvalues = numpy.linspace(0, 2 * 2 * math.pi, 1001)  # should be normalized when lots of features\n",
    "yvalues = 5 * numpy.sin(xvalues)\n",
    "ds = SupervisedDataSet(1, 1)\n",
    "for x, y in zip(xvalues, yvalues):\n",
    "    ds.addSample((x,), (y,))\n",
    "\n",
    "# Build the network\n",
    "net = buildNetwork(1,  # 1 input node\n",
    "                   30, # number of nodes in 1st hiddenlayer\n",
    "                   30, # number of nodes in 1st hiddenlayer\n",
    "                   1,  # 1 output node\n",
    "                   bias = True,\n",
    "                   hiddenclass = TanhLayer,  # better fit than sigmoid\n",
    "                   outclass = LinearLayer)    # for regression\n",
    "\n",
    "# Train the NN\n",
    "trainer = BackpropTrainer(net, ds, learningrate = 0.0005, weightdecay=0.001, momentum=0.5) #, verbose = True)   \n",
    "train_mse, validation_mse = trainer.trainUntilConvergence(maxEpochs = 50)\n",
    "\n",
    "# Evaluate:\n",
    "\n",
    "# Fitting error\n",
    "plt.scatter(xvalues, [ net.activate([x]) for x in xvalues ], linewidth = 2,\n",
    "           color = 'blue', label = 'NN output')\n",
    "plt.plot(xvalues, yvalues, linewidth = 2, color = 'red', label = 'target')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#Learning curves\n",
    "plt.clf()\n",
    "plt.plot(range(len(train_mse)), np.sqrt(train_mse), color='blue', label='training error')\n",
    "plt.plot(range(len(validation_mse)), np.sqrt(validation_mse), color='red', label='validation error')\n",
    "plt.title('Learning curves: loss(=RMSE) as a function of Epochs')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note on some of the parameters\n",
    "\n",
    "**weight decay** is the L2 regularization. 0 is no weight decay at all.\n",
    "\n",
    "**learning rate** gives the ratio of which parameters are changed into the direction of the gradient. The learning rate decreases by lrdecay, which is used to to multiply the learning rate after each training step. The parameters are also adjusted with respect to momentum, which is the ratio by which the gradient of the last timestep is used.  \n",
    "\n",
    "If **batchlearning** is set, the parameters are updated only at the end of each epoch. Default is False.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Other packages\n",
    "\n",
    "Initial analyses indicate that TensorFlow should bring also performance improvements compared to Theano, although no comprehensive benchmarks have yet been published.  \n",
    "As the other packages are out already for a while, they have large, active communities and often additional supporting software (examples are the very useful wrappers around Theano like Lasagne, Keras and Blocks that provider higher level abstractions to its engine)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:KS3]",
   "language": "python",
   "name": "conda-env-KS3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
