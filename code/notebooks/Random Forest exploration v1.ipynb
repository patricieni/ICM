{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "import numpy as np\n",
    "import itertools  \n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import time, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IMC_basedir = os.getcwd().split('code')[0]\n",
    "DATA_FILE = os.path.join(IMC_basedir, 'data/imputed_dataset_no_censoring_26022018_Amelia1.csv')\n",
    "DATA_MICE_FILE = os.path.join(IMC_basedir, 'data/imputed_dataset_no_censoring_26022018_MICE')\n",
    "TRAIN_FILE = os.path.join(IMC_basedir, 'data/amelia_train')\n",
    "TEST_FILE = os.path.join(IMC_basedir, 'data/amelia_test')\n",
    "MODEL_DIR = os.path.join(IMC_basedir, 'data/amelia_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Average Error: 0.4713 degrees.\n",
      "Accuracy = -inf%.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the Best Random Search Model\n",
    "best_random = rf_random.best_estimator_\n",
    "random_accuracy = evaluate(best_random, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improvement of nan%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paul.pop/miniconda3/envs/KS3/lib/python3.5/site-packages/ipykernel/__main__.py:1: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "rf = RandomForestRegressor(random_state = 42)\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, Y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "grid_accuracy = evaluate(best_grid, test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Categorical predictions\n",
    "from keras.utils.np_utils import to_categorical\n",
    "X = df.copy(deep=True)\n",
    "\n",
    "Y = np.array(to_categorical(X['life_expectancy_bin']))\n",
    "# remove columns\n",
    "X.drop('life_expectancy', axis = 1, inplace=True)\n",
    "X.drop('life_expectancy_bin', axis = 1, inplace=True)\n",
    "\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, Y = make_regression(n_features=54, n_informative=2,random_state=0, shuffle=False)\n",
    "\n",
    "regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "regr.fit(X_train, Y_train)\n",
    "\n",
    "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,\n",
    "           max_features='auto', max_leaf_nodes=None,\n",
    "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "           min_samples_leaf=1, min_samples_split=2,\n",
    "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
    "           oob_score=False, random_state=0, verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(regr.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paul.pop/miniconda3/envs/KS3/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-59f5ca4ed5ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Build the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msine_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0msine_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msine_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msine_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msine_X_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msine_y_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msine_y_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msine_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msine_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.33\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "from sknn.mlp import Regressor, Layer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Build the dataset\n",
    "sine_X = np.linspace(0, 2 * 2 * math.pi, 1001)  \n",
    "sine_y = 5 * np.sin(sine_X)\n",
    "sine_X_train, sine_X_test, sine_y_train, sine_y_test = train_test_split(sine_X, sine_y, test_size=0.33, random_state=42)\n",
    "# Transform 1D arrays to 2D arrays\n",
    "sine_X_train = sine_X_train[:,None]\n",
    "sine_X_test = sine_X_test[:,None]\n",
    "sine_y_train = sine_y_train[:,None]\n",
    "sine_y_test = sine_y_test[:,None]\n",
    "\n",
    "# Build the NN\n",
    "nn01 = Regressor(layers=[# input layer is added automatically, based on the number of features\n",
    "                        # a bias unit is added automatically for every input and hiddens layers\n",
    "                        Layer(\"Tanh\", units=30),      # 1st hidden layer: Rectifier, Sigmoid or Tanh\n",
    "                        Layer(\"Tanh\", units=30),      # 2nd hidden layer: Rectifier, Sigmoid or Tanh\n",
    "                        Layer(\"Linear\", units=1)],    # output layer                           \n",
    "                learning_rate=0.001,\n",
    "                weight_decay=0.1,                     # weight_decay = regularization\n",
    "                regularize='L2',\n",
    "                learning_momentum=0.66,\n",
    "                n_iter=50,\n",
    "                batch_size=1,                         # batch_size=1: each sample is treated on its own\n",
    "                loss_type='mse',\n",
    "                verbose=True)\n",
    "\n",
    "# Fitting the model\n",
    "nn01.fit(sine_X_train, sine_y_train)\n",
    "\n",
    "# Making the prediction\n",
    "sine_y_pred = nn01.predict(sine_X_test) \n",
    "\n",
    "# Results\n",
    "print(\"Results of SKNN Regression:\")\n",
    "print(\"Residual sum of squares (MSE): %.2f\" % np.mean((sine_y_pred - sine_y_test) ** 2))\n",
    "print('Variance score: %.2f' % nn01.score(sine_X_test, sine_y_test)) # Explained variance score: 1 is perfect prediction\n",
    "\n",
    "# Plot outputs\n",
    "plt.clf() \n",
    "plt.scatter(sine_X_train, sine_y_train,  color='black', label='training data', alpha=0.1)\n",
    "plt.scatter(sine_X_test, sine_y_test,  color='red', label='test data')\n",
    "plt.scatter(sine_X_test, sine_y_pred, color='blue', label='NN prediction') ## NN\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomized Search for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Specify parameters and distributions to sample from\n",
    "param_dist = {  \"learning_rate\"     : np.logspace(-4, -2, 3),  # 3 numbers from 1e-4 to 1e-2\n",
    "                \"weight_decay\"      : np.logspace(-4, -2, 3),\n",
    "                \"learning_momentum\" : [0.33, 0.66, 0.99],\n",
    "                \"n_iter\"            : [30, 40, 50]}\n",
    "\n",
    "# Start the Randomized Search\n",
    "n_iter_search = 20\n",
    "random_search = RandomizedSearchCV(nn01, param_distributions=param_dist, n_iter=n_iter_search, \n",
    "                                   scoring='mean_squared_error', n_jobs=-1, cv=3, verbose=3)\n",
    "\n",
    "random_search = random_search.fit(sine_X_train, sine_y_train)\n",
    "print \"Best parameters set found on development set:\"\n",
    "print random_search.best_score_, random_search.best_params_\n",
    "print\n",
    "print \"Grid scores on development set:\"\n",
    "for params, mean_score, scores in random_search.grid_scores_:\n",
    "    print \"%0.3f (+/-%0.03f) for %r\" % (mean_score, scores.std() * 2, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn01_opt = Regressor(layers=[# input layer is added automatically, based on the number of features\n",
    "                             # a bias unit is added automatically for every input and hiddens layers\n",
    "                             Layer(\"Tanh\", units=30),      # 1st hidden layer: Rectifier, Sigmoid or Tanh\n",
    "                             Layer(\"Tanh\", units=30),      # 2nd hidden layer: Rectifier, Sigmoid or Tanh\n",
    "                             Layer(\"Linear\", units=1)],    # output layer                           \n",
    "                learning_rate=random_search.best_params_['learning_rate'],  \n",
    "                weight_decay=random_search.best_params_['weight_decay'],      \n",
    "                regularize='L2',\n",
    "                learning_momentum=random_search.best_params_['learning_momentum'],\n",
    "                n_iter=random_search.best_params_['n_iter'],\n",
    "                batch_size=1,    # batch_size=1: each sample is treated on its own\n",
    "                loss_type='mse',\n",
    "                verbose=True)\n",
    "\n",
    "# Fitting the model\n",
    "nn01_opt.fit(sine_X_train, sine_y_train)\n",
    "\n",
    "# Plot outputs\n",
    "plt.clf() \n",
    "plt.scatter(sine_X_train, sine_y_train,  color='black', label='training data', alpha=0.1)\n",
    "plt.scatter(sine_X_test, sine_y_test,  color='red', label='test data')\n",
    "plt.scatter(sine_X_test, nn01_opt.predict(sine_X_test) , color='blue', label='NN prediction') ## NN\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print \"Residual sum of squares (MSE): %.2f\" % np.mean((nn01_opt.predict(sine_X_test) - sine_y_test) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title = 'Learning Curves (NN with tuned hyperparameters)'    \n",
    "estimator = nn01_opt # regressor with tuned hyperparameters \n",
    "plot_learning_curve(estimator, title, sine_X_train, sine_y_train, \n",
    "                    ylim=(-10., 0.), cv=5, n_jobs=-1, scoring='mean_squared_error')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: It outputs the negative of the MSE, as it always tries to maximize the score. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:KS3]",
   "language": "python",
   "name": "conda-env-KS3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
