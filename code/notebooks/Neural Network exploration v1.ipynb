{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of Neural Network libraries in Python:\n",
    "# Training a Feedforward Neural Network for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Benchmark: Logistic Regression (scikit-learn)\n",
    "    \n",
    "#### 2. Scikit-Neural Network (SKNN)\n",
    "    2.1 Diabetes dataset\n",
    "    2.2 Sine function example\n",
    "\n",
    "#### 3. PyBrain\n",
    "    3.1 Diabetes dataset - Part 1\n",
    "    3.2 Diabetes dataset - Part 2\n",
    "    3.3 Sine function example\n",
    "\n",
    "#### 4. Other packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paul.pop/miniconda3/envs/KS3/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/paul.pop/miniconda3/envs/KS3/lib/python3.5/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "import numpy as np\n",
    "import itertools  \n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.cross_validation import KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Activation, Dense, BatchNormalization, Dropout\n",
    "from keras import optimizers\n",
    "from keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import time, math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IMC_basedir = os.getcwd().split('code')[0]\n",
    "DATA_FILE = os.path.join(IMC_basedir, 'data/imputed_dataset_no_censoring_26022018_Amelia1.csv')\n",
    "DATA_MICE_FILE = os.path.join(IMC_basedir, 'data/imputed_dataset_no_censoring_26022018_MICE')\n",
    "TRAIN_FILE = os.path.join(IMC_basedir, 'data/amelia_train')\n",
    "TEST_FILE = os.path.join(IMC_basedir, 'data/amelia_test')\n",
    "MODEL_DIR = os.path.join(IMC_basedir, 'data/amelia_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binning(col, cut_points, labels=None):\n",
    "    #Define min and max values:\n",
    "    minval = col.min()\n",
    "    maxval = col.max()\n",
    "\n",
    "    #create list by adding min and max to cut_points\n",
    "    break_points = [minval] + cut_points + [maxval]\n",
    "\n",
    "    # if no labels provided, use default labels 0 ... (n-1)\n",
    "    if not labels:\n",
    "        labels = range(len(cut_points)+1)\n",
    "\n",
    "    #Binning using cut function of pandas\n",
    "    colBin = pd.cut(col,bins=break_points,labels=labels,include_lowest=True)\n",
    "    return colBin\n",
    "\n",
    "def plot_report(Y_true, Y_pred):\n",
    "\n",
    "    # Plot non-normalized confusion matrix\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(confusion_matrix(Y_test, Y_pred), \n",
    "                          classes=labels, title='Confusion matrix')\n",
    "\n",
    "    print(classification_report(Y_test, Y_pred, target_names=labels))\n",
    "    print('Accuracy: {}'.format(accuracy_score(Y_test, Y_pred)))\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "def plot_hist(history, save_as=None):\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    if save_as:\n",
    "        plt.savefig(save_as + '_acc.jpg')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    if save_as:\n",
    "        plt.savefig(save_as + '_loss.jpg')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_mice = pd.read_csv(DATA_MICE_FILE)\n",
    "df_mice.drop(\"Unnamed: 0\", axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df_amelia = pd.read_csv(DATA_FILE)\n",
    "df_amelia.drop(\"Unnamed: 0\", axis = 1, inplace=True)\n",
    "labels = [\"1.5year\",\"4years\",\"more\"]\n",
    "cut_points = [500,1500]\n",
    "\n",
    "#labels = [\"3_months\",\"6_months\",\"9_months\",\"12_months\",\"15_months\",\"18_months\",\"2_years\",\"3_years\",\"4_years\",\"5_years\",\"10_years\",\"10_plus_years\"]\n",
    "#cut_points = [90,180,270,360,450,540,720,1095,1460,1825,3650]\n",
    "df_amelia.loc[:,\"life_expectancy_bin\"] = binning(df_amelia.life_expectancy, cut_points, labels)\n",
    "\n",
    "df_amelia['life_expectancy_bin'] = LabelEncoder().fit_transform(df_amelia['life_expectancy_bin'])\n",
    "#df_amelia.drop(\"life_expectancy\", axis=1, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "le_dict = dict() # Initialise an empty dictionary to keep all LabelEncoders\n",
    "df_categories = df_amelia.copy(deep=True) \n",
    "# Loop over attributes by excluding the continuous oness\n",
    "for column in df_categories.drop(['Age_surgery', 'life_expectancy', 'Tumor_grade','IDH_TERT','IK'], axis=1):  \n",
    "    le = LabelEncoder().fit(df_categories[column]) # Initialise the LabelEncoder and fit\n",
    "    df_categories[column] = le.transform(df_categories[column]) # Transform data and save in credit_clean DataFrame\n",
    "    le_dict[column] = le # Store the LabelEncdoer in dictionary\n",
    "    \n",
    "df = df_amelia.copy(deep=True)\n",
    "non_dummy_cols = ['Tumor_grade','IDH_TERT','life_expectancy','life_expectancy_bin','Gender','IK','Age_surgery']\n",
    "dummy_cols = list(set(df.columns) - set(non_dummy_cols))\n",
    "\n",
    "df = pd.get_dummies(df,columns=dummy_cols)\n",
    "\n",
    "df.Gender.replace(to_replace={'M':1, 'F':0},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_train_test_data(data_df, train_size=0.8):\n",
    "    df_tensorflow = data_df.copy(deep=True)\n",
    "    X_train, X_test = train_test_split(df_tensorflow, train_size=train_size, \n",
    "                                       test_size=1-train_size)\n",
    "\n",
    "    # remove columns\n",
    "    X_train.drop('life_expectancy', axis = 1, inplace=True)\n",
    "    X_test.drop('life_expectancy', axis = 1, inplace=True)\n",
    "\n",
    "    Y_train = X_train['life_expectancy_bin']\n",
    "    X_train.drop('life_expectancy_bin', axis = 1, inplace=True)\n",
    "\n",
    "    Y_test = X_test['life_expectancy_bin']\n",
    "    X_test.drop('life_expectancy_bin', axis = 1, inplace=True)\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Classification: Benchmark Logistic Regression (Scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = get_train_test_data(df, train_size=0.8)\n",
    "\n",
    "# print(X_train.shape, X_test.shape)\n",
    "# X_train.head(2)\n",
    "\n",
    "regr = linear_model.LogisticRegression()              # Create linear regression object\n",
    "regr.fit(X_train, Y_train) \n",
    "\n",
    "Y_pred = regr.predict(X_test)\n",
    "            \n",
    "plot_report(Y_test, Y_pred)\n",
    "\n",
    "# print ('Coefficients:', regr.coef_, regr.intercept_ )                # The coefficients\n",
    "print(\"Mean square error (MSE): %.2f\"\n",
    "      % np.mean((Y_pred - Y_test) ** 2))      # The mean square error\n",
    "print ('R^2: %.2f' % regr.score(X_test, Y_test) )  # Explained variance score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Classification: Keras NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df.copy(deep=True)\n",
    "\n",
    "Y = np.array(X['life_expectancy_bin'])\n",
    "# remove columns\n",
    "X.drop('life_expectancy', axis = 1, inplace=True)\n",
    "X.drop('life_expectancy_bin', axis = 1, inplace=True)\n",
    "\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define baseline model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=54, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=100, batch_size=5, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = cross_val_score(estimator, X, Y, cv=KFold(n=X.shape[0], n_folds=2, shuffle=True))\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Regression: Keras NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_kfolds(regressor):\n",
    "    for k_folds in [5, 10, 15]:\n",
    "        # evaluate model with standardized dataset\n",
    "        np.random.seed(seed)\n",
    "        estimators = []\n",
    "        #estimators.append(('standardize', StandardScaler()))\n",
    "        estimators.append(('mlp', regressor))\n",
    "        pipeline = Pipeline(estimators)\n",
    "        kfold = KFold(n=X.shape[0], n_folds=k_folds)\n",
    "        results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
    "        print(\"K_folds {}: \\nMean {} \\nStd {}\".format(k_folds, results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = df.copy(deep=True)\n",
    "\n",
    "Y = np.array(X['life_expectancy_bin'])\n",
    "# remove columns\n",
    "X.drop('life_expectancy', axis = 1, inplace=True)\n",
    "X.drop('life_expectancy_bin', axis = 1, inplace=True)\n",
    "\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define base model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim=54, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_kfolds(KerasRegressor(build_fn=baseline_model, nb_epoch=50, batch_size=5, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def larger_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, kernel_initializer=\"normal\", input_dim=54, activation=\"relu\"))\n",
    "    model.add(Dense(6, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test_kfolds(KerasRegressor(build_fn=larger_model, nb_epoch=50, batch_size=5, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wider_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=54, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_kfolds(KerasRegressor(build_fn=wider_model, nb_epoch=50, batch_size=5, verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mlp_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(50, input_dim = 54, kernel_initializer='he_normal', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))    \n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(50, kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, kernel_initializer='he_normal'))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    adam = optimizers.Adam(lr = 0.001)\n",
    "    model.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# create 5 models to ensemble\n",
    "model1 = KerasClassifier(build_fn = mlp_model, epochs = 100)\n",
    "model2 = KerasClassifier(build_fn = mlp_model, epochs = 100)\n",
    "model3 = KerasClassifier(build_fn = mlp_model, epochs = 100)\n",
    "model4 = KerasClassifier(build_fn = mlp_model, epochs = 100)\n",
    "model5 = KerasClassifier(build_fn = mlp_model, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ensemble_clf = VotingClassifier(estimators = [('model1', model1), ('model2', model2), ('model3', model3), ('model4', model4), ('model5', model5)], voting = 'soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ensemble_clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, Y_train, X_test, Y_test = get_train_test_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, kernel_initializer=\"normal\", input_dim=54))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "    \n",
    "model.compile(optimizer = optimizers.SGD(lr = 0.001), loss = 'mean_squared_error', metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(np.array(X_train), np.array(Y_train), epochs = 100, verbose = 1)\n",
    "\n",
    "# plt.plot(history.history['acc'])\n",
    "# plt.plot(history.history['val_acc'])\n",
    "# plt.legend(['training', 'validation'], loc = 'upper left')\n",
    "# plt.show()\n",
    "\n",
    "results = model.evaluate(X_test, Y_test)\n",
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Categorical predictions\n",
    "from keras.utils.np_utils import to_categorical\n",
    "X = df.copy(deep=True)\n",
    "\n",
    "Y = np.array(to_categorical(X['life_expectancy_bin']))\n",
    "# remove columns\n",
    "X.drop('life_expectancy', axis = 1, inplace=True)\n",
    "X.drop('life_expectancy_bin', axis = 1, inplace=True)\n",
    "\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "model = Sequential()\n",
    "# model.add(Dense(108, input_shape=(54,), kernel_initializer='normal', activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(32, kernel_initializer='normal', activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(16, kernel_initializer='normal', activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(8, kernel_initializer='normal', activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.add(Dense(64, activation='relu', input_dim=54))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# model.add(Dense(input_dim=54, output_dim=12, activation='relu'))\n",
    "# model.add(Dropout(0.1))\n",
    "# model.add(Dense(input_dim=12, output_dim=12, activation='relu'))\n",
    "# model.add(Dropout(0.1))\n",
    "# model.add(Dense(output_dim=1, activation='softmax'))\n",
    "# model.compile(loss='mean_squared_error', optimizer='adadelta', metrics=['accuracy'])\n",
    "\n",
    "# 3\n",
    "#tbCallBack = keras.callbacks.TensorBoard(log_dir='/tmp/keras_logs', write_graph=True)\n",
    "\n",
    "# 4\n",
    "#model.compile(loss='mean_squared_error', optimizer='adadelta', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "hist = model.fit(X, Y, epochs=600, batch_size=128,  verbose=1, validation_split=0.4)#, callbacks=[tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_hist(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_hist(hist) #, save_as='deep_5_wide_108')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scikit-Neural Network (SKNN)\n",
    "\n",
    "Different approach for Classification vs. Regression using Neural Network:\n",
    "- Training examples: Rn x {class_1, ..., class_n} (one-hot encoding) vs Rn x Rm\n",
    "- Last layer: softmax vs linear / sigmoid\n",
    "- Loss function: Cross entropy vs MSE / Absolute error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. SKNN: Regression (sine function example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, Y = make_regression(n_features=54, n_informative=2,random_state=0, shuffle=False)\n",
    "\n",
    "regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
    "regr.fit(X_train, Y_train)\n",
    "\n",
    "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,\n",
    "           max_features='auto', max_leaf_nodes=None,\n",
    "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "           min_samples_leaf=1, min_samples_split=2,\n",
    "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
    "           oob_score=False, random_state=0, verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(regr.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'downsample'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-59f5ca4ed5ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Fitting the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mnn01\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msine_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msine_y_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Making the prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/paul.pop/miniconda3/envs/KS3/lib/python3.5/site-packages/sknn/mlp.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, w)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRegressor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/paul.pop/miniconda3/envs/KS3/lib/python3.5/site-packages/sknn/mlp.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, w)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training on dataset of {:,} samples with {} total size.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/paul.pop/miniconda3/envs/KS3/lib/python3.5/site-packages/sknn/mlp.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, X, y, w)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_specs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiLayerPerceptronBackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/paul.pop/miniconda3/envs/KS3/lib/python3.5/site-packages/sknn/backend/__init__.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"No backend for module sknn was imported.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/paul.pop/miniconda3/envs/KS3/lib/python3.5/site-packages/sknn/backend/lasagne/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultiLayerPerceptronBackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Register this implementation as the MLP backend.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/paul.pop/miniconda3/envs/KS3/lib/python3.5/site-packages/sknn/backend/lasagne/mlp.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonlinearities\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/paul.pop/miniconda3/envs/KS3/lib/python3.5/site-packages/lasagne/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnonlinearities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mobjectives\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/paul.pop/miniconda3/envs/KS3/lib/python3.5/site-packages/lasagne/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/paul.pop/miniconda3/envs/KS3/lib/python3.5/site-packages/lasagne/layers/pool.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mas_tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownsample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'downsample'"
     ]
    }
   ],
   "source": [
    "from sknn.mlp import Regressor, Layer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Build the dataset\n",
    "sine_X = np.linspace(0, 2 * 2 * math.pi, 1001)  \n",
    "sine_y = 5 * np.sin(sine_X)\n",
    "sine_X_train, sine_X_test, sine_y_train, sine_y_test = train_test_split(sine_X, sine_y, test_size=0.33, random_state=42)\n",
    "# Transform 1D arrays to 2D arrays\n",
    "sine_X_train = sine_X_train[:,None]\n",
    "sine_X_test = sine_X_test[:,None]\n",
    "sine_y_train = sine_y_train[:,None]\n",
    "sine_y_test = sine_y_test[:,None]\n",
    "\n",
    "# Build the NN\n",
    "nn01 = Regressor(layers=[# input layer is added automatically, based on the number of features\n",
    "                        # a bias unit is added automatically for every input and hiddens layers\n",
    "                        Layer(\"Tanh\", units=30),      # 1st hidden layer: Rectifier, Sigmoid or Tanh\n",
    "                        Layer(\"Tanh\", units=30),      # 2nd hidden layer: Rectifier, Sigmoid or Tanh\n",
    "                        Layer(\"Linear\", units=1)],    # output layer                           \n",
    "                learning_rate=0.001,\n",
    "                weight_decay=0.1,                     # weight_decay = regularization\n",
    "                regularize='L2',\n",
    "                learning_momentum=0.66,\n",
    "                n_iter=50,\n",
    "                batch_size=1,                         # batch_size=1: each sample is treated on its own\n",
    "                loss_type='mse',\n",
    "                verbose=True)\n",
    "\n",
    "# Fitting the model\n",
    "nn01.fit(sine_X_train, sine_y_train)\n",
    "\n",
    "# Making the prediction\n",
    "sine_y_pred = nn01.predict(sine_X_test) \n",
    "\n",
    "# Results\n",
    "print(\"Results of SKNN Regression:\")\n",
    "print(\"Residual sum of squares (MSE): %.2f\" % np.mean((sine_y_pred - sine_y_test) ** 2))\n",
    "print('Variance score: %.2f' % nn01.score(sine_X_test, sine_y_test)) # Explained variance score: 1 is perfect prediction\n",
    "\n",
    "# Plot outputs\n",
    "plt.clf() \n",
    "plt.scatter(sine_X_train, sine_y_train,  color='black', label='training data', alpha=0.1)\n",
    "plt.scatter(sine_X_test, sine_y_test,  color='red', label='test data')\n",
    "plt.scatter(sine_X_test, sine_y_pred, color='blue', label='NN prediction') ## NN\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomized Search for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Specify parameters and distributions to sample from\n",
    "param_dist = {  \"learning_rate\"     : np.logspace(-4, -2, 3),  # 3 numbers from 1e-4 to 1e-2\n",
    "                \"weight_decay\"      : np.logspace(-4, -2, 3),\n",
    "                \"learning_momentum\" : [0.33, 0.66, 0.99],\n",
    "                \"n_iter\"            : [30, 40, 50]}\n",
    "\n",
    "# Start the Randomized Search\n",
    "n_iter_search = 20\n",
    "random_search = RandomizedSearchCV(nn01, param_distributions=param_dist, n_iter=n_iter_search, \n",
    "                                   scoring='mean_squared_error', n_jobs=-1, cv=3, verbose=3)\n",
    "\n",
    "random_search = random_search.fit(sine_X_train, sine_y_train)\n",
    "print \"Best parameters set found on development set:\"\n",
    "print random_search.best_score_, random_search.best_params_\n",
    "print\n",
    "print \"Grid scores on development set:\"\n",
    "for params, mean_score, scores in random_search.grid_scores_:\n",
    "    print \"%0.3f (+/-%0.03f) for %r\" % (mean_score, scores.std() * 2, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn01_opt = Regressor(layers=[# input layer is added automatically, based on the number of features\n",
    "                             # a bias unit is added automatically for every input and hiddens layers\n",
    "                             Layer(\"Tanh\", units=30),      # 1st hidden layer: Rectifier, Sigmoid or Tanh\n",
    "                             Layer(\"Tanh\", units=30),      # 2nd hidden layer: Rectifier, Sigmoid or Tanh\n",
    "                             Layer(\"Linear\", units=1)],    # output layer                           \n",
    "                learning_rate=random_search.best_params_['learning_rate'],  \n",
    "                weight_decay=random_search.best_params_['weight_decay'],      \n",
    "                regularize='L2',\n",
    "                learning_momentum=random_search.best_params_['learning_momentum'],\n",
    "                n_iter=random_search.best_params_['n_iter'],\n",
    "                batch_size=1,    # batch_size=1: each sample is treated on its own\n",
    "                loss_type='mse',\n",
    "                verbose=True)\n",
    "\n",
    "# Fitting the model\n",
    "nn01_opt.fit(sine_X_train, sine_y_train)\n",
    "\n",
    "# Plot outputs\n",
    "plt.clf() \n",
    "plt.scatter(sine_X_train, sine_y_train,  color='black', label='training data', alpha=0.1)\n",
    "plt.scatter(sine_X_test, sine_y_test,  color='red', label='test data')\n",
    "plt.scatter(sine_X_test, nn01_opt.predict(sine_X_test) , color='blue', label='NN prediction') ## NN\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print \"Residual sum of squares (MSE): %.2f\" % np.mean((nn01_opt.predict(sine_X_test) - sine_y_test) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title = 'Learning Curves (NN with tuned hyperparameters)'    \n",
    "estimator = nn01_opt # regressor with tuned hyperparameters \n",
    "plot_learning_curve(estimator, title, sine_X_train, sine_y_train, \n",
    "                    ylim=(-10., 0.), cv=5, n_jobs=-1, scoring='mean_squared_error')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: It outputs the negative of the MSE, as it always tries to maximize the score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pybrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pip install git+https://github.com/pybrain/pybrain.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 PyBrain: Regression (diabetes dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pybrain2\n",
    "from pybrain.structure import SigmoidLayer, LinearLayer, TanhLayer\n",
    "from pybrain.datasets import SupervisedDataSet\n",
    "from pybrain.supervised.trainers import BackpropTrainer\n",
    "import pybrain.tools.shortcuts as pb\n",
    "import numpy, math\n",
    "\n",
    "# Build the dataset\n",
    "xvalues = diabetes_X_train[:,0]   #should be normalized when lots of features\n",
    "yvalues = diabetes_y_train\n",
    "ds = SupervisedDataSet(1, 1)\n",
    "for x, y in zip(xvalues, yvalues):\n",
    "    ds.addSample((x), (y))\n",
    "    \n",
    "# Build the NN\n",
    "nn1 = pb.buildNetwork(1,  # 1 input node\n",
    "                   20,    # number of nodes in 1st hidden layer\n",
    "                   #10,   # number of nodes in 2nd hidden layer\n",
    "                   1,     # 1 output node\n",
    "                   bias = False,\n",
    "                   hiddenclass = SigmoidLayer,\n",
    "                   outclass = LinearLayer )\n",
    "\n",
    "# Train the NN\n",
    "trainer = BackpropTrainer(nn1, ds, learningrate = 0.01, weightdecay=0.01, momentum=0.6) #, verbose = True)\n",
    "train_mse, validation_mse = trainer.trainUntilConvergence(maxEpochs = 20, continueEpochs=5, validationProportion=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note on some of the parameters\n",
    "\n",
    "**validationProportion**: ratio of the dataset that is used for the validation dataset.\n",
    "If maxEpochs is given, at most that many epochs are trained. Each time validation error hits a minimum, try for continueEpochs # epochs to find a better one.\n",
    "\n",
    "**Epoch**: one epoch means that every example has been seen once. It is preferable to track epochs rather than iterations since \n",
    "the number of iterations depends on the arbitrary setting of batch size. Batchs are used for example in the minibatch method,\n",
    "for example, for 1000 examples, the NN is trained on examples 1-100, then examples 101-201, etc.\n",
    "\n",
    "**Momentum**: 0 < m < 1 is a global parameter which must be determined by trial and error. Momentum simply adds a fraction m of the previous weight update to the current one. When the gradient keeps pointing in the same direction, this will increase the size of the steps taken towards the minimum. It is otherefore often necessary to reduce the global learning rate Âµ when using a lot of momentum (m close to 1). If you combine a high learning rate with a lot of momentum, you will rush past the minimum with huge steps! When the gradient keeps changing direction, momentum will smooth out the variations. Adding a momentum can help to speed up convergence to the minimum by damping oscillations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make the prediction\n",
    "y_pred = [ nn1.activate([x]) for x in diabetes_X_test ]  \n",
    "\n",
    "# Print the weights\n",
    "def weight_connection(n):\n",
    "    for mod in n.modules:\n",
    "        for conn in n.connections[mod]:\n",
    "            print conn\n",
    "            for cc in range(len(conn.params)):\n",
    "                print conn.whichBuffers(cc), conn.params[cc]\n",
    "#weight_connection(nn1)\n",
    "\n",
    "# And evaluate:\n",
    "\n",
    "# Fitting error\n",
    "plt.clf() \n",
    "plt.scatter(diabetes_X_train, diabetes_y_train,  color='black', alpha=0.1)\n",
    "plt.scatter(diabetes_X_test, diabetes_y_test,  color='red', label = 'target')\n",
    "plt.scatter(diabetes_X_test, y_pred, color = 'blue', label = 'NN output')  # NN\n",
    "plt.scatter(diabetes_X_test, diabetes_y_pred_lin, color='green', label='linear model')  # benchmark\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Learning curves\n",
    "plt.clf()\n",
    "plt.plot(range(len(train_mse)), np.sqrt(train_mse), color='blue', label='training error')\n",
    "plt.plot(range(len(validation_mse)), np.sqrt(validation_mse), color='red', label='validation error')\n",
    "plt.title('Learning curves: loss(=RMSE) as a function of Epochs')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 PyBrain: Regression (diabetes dataset)\n",
    "\n",
    "Similar as above (section 3.1), except here we are creating the feeforward neural network from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pybrain.structure import FeedForwardNetwork, LinearLayer, SigmoidLayer, TanhLayer, BiasUnit, FullConnection\n",
    "from pybrain.supervised.trainers import BackpropTrainer\n",
    "\n",
    "nn2 = FeedForwardNetwork()\n",
    "\n",
    "bias = False\n",
    "\n",
    "\n",
    "## Constructing the input, hidden and output layers:\n",
    "\n",
    "# In order to use them, we have to add them to the network:\n",
    "nn2.addInputModule( LinearLayer(1, name = 'in') )      # inLayer\n",
    "nn2.addModule( SigmoidLayer(5, name = 'hidden0') )     # hiddenLayer\n",
    "nn2.addOutputModule( LinearLayer(1, name = 'out') )    # outLayer\n",
    "if bias:\n",
    "    nn2.addInputModule( BiasUnit(name = 'inbias') )        # bias for input layer\n",
    "    nn2.addModule( BiasUnit(name = 'bias0') )              # bias for hidden layer\n",
    "\n",
    "# As with modules, we have to explicitly add them to the network:\n",
    "theta1 = FullConnection(nn2['in'], nn2['hidden0'])\n",
    "theta2 = FullConnection(nn2['hidden0'], nn2['out'])\n",
    "nn2.addConnection( theta1 )    # in_to_hidden connections\n",
    "nn2.addConnection( theta2 )    # hidden_to_out connections\n",
    "if bias:\n",
    "    nn2.addConnection(FullConnection(nn2['inbias'], nn2['hidden0']))\n",
    "    nn2.addConnection(FullConnection(nn2['bias0'], nn2['out']))\n",
    "\n",
    "nn2.sortModules()     # making the MLP usable\n",
    "\n",
    "\n",
    "# Build the dataset ***with bias units***\n",
    "xvalues = diabetes_X_train[:,0]   # Should be normalized when lots of features\n",
    "yvalues = diabetes_y_train\n",
    "ds2 = SupervisedDataSet(1, 1)     # Dataset for Supervised Regression Training\n",
    "# No need to add here the bias term to the feature matrix: it will be added in the training method\n",
    "for x, y in zip(xvalues, yvalues):\n",
    "    ds2.addSample((x), (y))       # ds.addSample((x1, ...., xn), (y)) # for each training example\n",
    "\n",
    "# shows the nn2 weights\n",
    "weight_connection(nn2)\n",
    "    \n",
    "# Train the NN\n",
    "trainer = BackpropTrainer(nn2, ds2, learningrate = 0.01, weightdecay=0.01, momentum=0.0) #, verbose = True)\n",
    "train_mse, validation_mse = trainer.trainUntilConvergence(maxEpochs = 20, continueEpochs=5, validationProportion=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make the predictions\n",
    "y_pred = [ nn2.activate([x]) for x in diabetes_X_test ]  \n",
    "\n",
    "# Evaluate: \n",
    "\n",
    "# Plot outputs\n",
    "plt.clf() \n",
    "plt.scatter(diabetes_X_train, diabetes_y_train,  color='black', alpha=0.1)\n",
    "plt.scatter(diabetes_X_test, diabetes_y_test,  color='red', label = 'target')\n",
    "plt.scatter(diabetes_X_test, y_pred, color = 'blue', label = 'NN output')\n",
    "plt.scatter(diabetes_X_test, diabetes_y_pred_lin, color='green', label='linear model')  # benchmark\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Learning curves\n",
    "plt.clf()\n",
    "plt.plot(range(len(train_mse)), np.sqrt(train_mse), color='blue', label='training error')\n",
    "plt.plot(range(len(validation_mse)), np.sqrt(validation_mse), color='red', label='validation error')\n",
    "plt.title('Learning curves: loss(=RMSE) as a function of Epochs')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 PyBrain: Regression (sine function example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'pybrain'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d5302d0f77ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpybrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructure\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSigmoidLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinearLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTanhLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpybrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortcuts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuildNetwork\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpybrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSupervisedDataSet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpybrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupervised\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBackpropTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'pybrain'"
     ]
    }
   ],
   "source": [
    "from pybrain.structure import SigmoidLayer, LinearLayer, TanhLayer\n",
    "from pybrain.tools.shortcuts import buildNetwork\n",
    "from pybrain.datasets import SupervisedDataSet\n",
    "from pybrain.supervised.trainers import BackpropTrainer\n",
    "\n",
    "# Build the dataset\n",
    "xvalues = numpy.linspace(0, 2 * 2 * math.pi, 1001)  # should be normalized when lots of features\n",
    "yvalues = 5 * numpy.sin(xvalues)\n",
    "ds = SupervisedDataSet(1, 1)\n",
    "for x, y in zip(xvalues, yvalues):\n",
    "    ds.addSample((x,), (y,))\n",
    "\n",
    "# Build the network\n",
    "net = buildNetwork(1,  # 1 input node\n",
    "                   30, # number of nodes in 1st hiddenlayer\n",
    "                   30, # number of nodes in 1st hiddenlayer\n",
    "                   1,  # 1 output node\n",
    "                   bias = True,\n",
    "                   hiddenclass = TanhLayer,  # better fit than sigmoid\n",
    "                   outclass = LinearLayer)    # for regression\n",
    "\n",
    "# Train the NN\n",
    "trainer = BackpropTrainer(net, ds, learningrate = 0.0005, weightdecay=0.001, momentum=0.5) #, verbose = True)   \n",
    "train_mse, validation_mse = trainer.trainUntilConvergence(maxEpochs = 50)\n",
    "\n",
    "# Evaluate:\n",
    "\n",
    "# Fitting error\n",
    "plt.scatter(xvalues, [ net.activate([x]) for x in xvalues ], linewidth = 2,\n",
    "           color = 'blue', label = 'NN output')\n",
    "plt.plot(xvalues, yvalues, linewidth = 2, color = 'red', label = 'target')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#Learning curves\n",
    "plt.clf()\n",
    "plt.plot(range(len(train_mse)), np.sqrt(train_mse), color='blue', label='training error')\n",
    "plt.plot(range(len(validation_mse)), np.sqrt(validation_mse), color='red', label='validation error')\n",
    "plt.title('Learning curves: loss(=RMSE) as a function of Epochs')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note on some of the parameters\n",
    "\n",
    "**weight decay** is the L2 regularization. 0 is no weight decay at all.\n",
    "\n",
    "**learning rate** gives the ratio of which parameters are changed into the direction of the gradient. The learning rate decreases by lrdecay, which is used to to multiply the learning rate after each training step. The parameters are also adjusted with respect to momentum, which is the ratio by which the gradient of the last timestep is used.  \n",
    "\n",
    "If **batchlearning** is set, the parameters are updated only at the end of each epoch. Default is False.  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:KS3]",
   "language": "python",
   "name": "conda-env-KS3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
