{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "As usual, we start by visualizing our missing data. \n",
    "\n",
    "We started deadset on sticking to a pure Python workflow. If something not implemented in Python, we would rather code it ourselves than revert to R. That’s how we roll.\n",
    "\n",
    "It became clear over time that R keeps a tremendous advantage in esoteric statistical fields such as handling missing data. At this point in time, no robust and comprehensive python package for handling missing data. A good lesson in data science – be versatile and use whatever is available.\n",
    "\n",
    "I will show you here the workflow by appealing to the best R packages I could find for each task.\n",
    "\n",
    "Visualization is key. Important not only for you, but also for your audience/publications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in parse(text = x, srcfile = src): <text>:1:1: unexpected input\n1: %load_ext rpy2.ipython\n    ^\n",
     "output_type": "error",
     "traceback": [
      "Error in parse(text = x, srcfile = src): <text>:1:1: unexpected input\n1: %load_ext rpy2.ipython\n    ^\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "%load_ext rpy2.ipython\n",
    "\n",
    "%R require(ggplot)\n",
    "df = pd.DataFrame({'Alphabet': ['a', 'b', 'c', 'd','e', 'f', 'g', 'h','i'],\n",
    "                   'A': [4, 3, 5, 2, 1, 7, 7, 5, 9],\n",
    "                   'B': [0, 4, 3, 6, 7, 10,11, 9, 13],\n",
    "                   'C': [1, 2, 3, 1, 2, 3, 1, 2, 3]})\n",
    "%R -i df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change to directory necessary \n",
    "# We tell R that missing values are coded by absence of any symbol (na.strong = ''), \n",
    "# and that the dataset has a header\n",
    "\n",
    "# Load the data\n",
    "# Patric \n",
    "data <- read.csv(\"../.././data/imputation_dataset_no_censoring_24022018.csv\", na.strings = '', header=TRUE)\n",
    "\n",
    "#Alex\n",
    "#setwd(\"Dropbox/Healthcare work Patric\")\n",
    "#data <- read.csv(\"imputation_dataset_no_censoring_24022018\", na.strings = '', header=TRUE)\n",
    "#data_knn_part <- kNN(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in matrixplot(data, labels = TRUE): could not find function \"matrixplot\"\n",
     "output_type": "error",
     "traceback": [
      "Error in matrixplot(data, labels = TRUE): could not find function \"matrixplot\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "matrixplot(data, labels=TRUE) # Doesn't work, what do we need to load?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run me!\n",
    "![title](img/knn_matrixplot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in aggr(data, prop = c(TRUE, FALSE), sortVars = TRUE, sortCombs = TRUE): could not find function \"aggr\"\n",
     "output_type": "error",
     "traceback": [
      "Error in aggr(data, prop = c(TRUE, FALSE), sortVars = TRUE, sortCombs = TRUE): could not find function \"aggr\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "# R Code\n",
    "aggr(data, prop=c(TRUE,FALSE), sortVars=TRUE, sortCombs=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run me! \n",
    "![title](img/knn_proportions_missing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VIM package was initially developed for data imputation. However, its real strength is in missing data visualization.\n",
    "ADD LINK TO VIM PACKAGE FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in library(VIM): there is no package called ‘VIM’\n",
     "output_type": "error",
     "traceback": [
      "Error in library(VIM): there is no package called ‘VIM’\nTraceback:\n",
      "1. library(VIM)",
      "2. stop(txt, domain = NA)"
     ]
    }
   ],
   "source": [
    "#load VIM library for missing data visualization\n",
    "library(VIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrixplot(data, labels=TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This is a high level visualization of our observed vs missing observations across our dataset.\n",
    "In red, we have missing values across our variables, and in black, observed values across our variables.\n",
    "Here we look for the magnitude of missing data per variable, and for patterns to investigate further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's quantify the number of missing values per variable\n",
    "summary(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's try a frequency plot of missing values per variable\n",
    "aggr(dataset, prop = F, numbers = T)\n",
    "\n",
    "#explain prop and numbers options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This type of visualization gives us two things:\n",
    "1. it ranks for us the variables in our dataset by proportion of missing observation (left part)\n",
    "2. and especially it allows us to see patterns of joint missingness among variables (right part)\n",
    "\n",
    "# blue refers to observed data and red to missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to narrow down on a pair of variables. A good idea is to test various variables with high proportion of missing data, against the response variable in your study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming two quantitative variables, you draw a margin plot. Like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marginplot(dataset[,c(\"IK\",\"IDH_TERT\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "blue box plot: distribution of values of a variable, given the other variable is observed\n",
    "red box plot: distribution of values of a variable, given the other variable is missing\n",
    "If your data is MCAR, you expect the blue and red box plots to be identical\n",
    "This is rarely the case\n",
    "If your data is MAR, we expect them to be similar, but not the same\n",
    "In comparing the blue and red box plots, focus on the mean first, and the dispersion around the mean second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming a quantitative and a categorical variable, you draw a box plot. Like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbox(data[c(\"life_expectancy\", \"IDH\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "If interested, you can also use VIM for scatterplots as\n",
    " # scattMiss(nhanes[,c(\"bmi\",\"chl\")], inEllipse=TRUE, col=mdc(1:2),alpha=.8,bty=\"n\",\n",
    " interactive=TRUE, axes=TRUE, lwd=c(1.5,1.5), pch=19, cex.lab=1.1, cex.axis=.9)\n",
    " # rugNA(nhanes[,c(\"bmi\")], nhanes[,c(\"chl\")], side=2, col=mdc(2), lwd=1.5)\n",
    " # rugNA(nhanes[,c(\"bmi\")], nhanes[,c(\"chl\")], side=1, col=mdc(2), lwd=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What other ways of exploring/describing missing data do we have?\n",
    "\n",
    "1. Counts by reason of missingness\n",
    "2. Kaplan-Meier plot by treatment group of the probability of remaining in the study over time\n",
    "3. Plot of mean outcome over time by selected categories of discontinuation i.e. subjects discontinuing showing a worse outcome at that point than completers\n",
    "4. Spaghetti plots – plots of outcome over time with one line per subject (randomly selected or completers vs drop outs…)\n",
    "5. Logistic regression with missingness at end of study as the dependent variable. Investigating variables that may explain the missingness and how they are related to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Complete case analysis\n",
    "= Deleting all observations that have any missing values.\n",
    "One of most common methods used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problems:\n",
    "produces biased estimated (Bell et al. 2013)\n",
    "loss of information and power as a result of all the observations hence deleted\n",
    "\n",
    "Exception: works under MCAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Available case analysis\n",
    "= Taking your incomplete dataset with missing values, and for the variable of interest, using all the observations that do not have missing values for that variable.\n",
    "In practice this means that in the same study you will use different subsamples from your dataset for different variables of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the N.E.R.D.S:\n",
    "\n",
    "Whaaaaat?\n",
    "\n",
    "In linear regression we know that a regression can be estimated using only either the sample means and covariance matrix, or the means, standard deviations and correlation matrix (see any introductory statistical book such as Introduction to Statistical Learning). Pairwise deletion aims to take advantage of this insight, computing these statistics using all the cases for which data is available, across our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, same problems as with complete case analysis.\n",
    "Exception: works under MCAR but analysis needs modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A large and esoteric debate among missing data statisticians as to which of the two is better under which scenarios. Recommend to avoid this debate alltogether."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If what I've done my whole life was wrong, what can I do now to atone for my sins?\n",
    "Impute baby, impute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is imputation?\n",
    "Imputation amounts to filling in missing values with appropriate estimates for them, and then using standard complete data methods to study the now complete dataset. “From an operational standpoint, imputation solves the missing-data problem at the outset, enabling the analyst to proceed without further hindrance.” (Schafer, 1999)\n",
    "\n",
    "It is important to understand from the beginning the objective of imputation. It is not to estimate as accurately as possible any single missing value. The objective is to create a complete dataset that preserves as much as possible the characteristics of the original complete dataset. Intuitively this makes sense because in applied statistics, we aim to infer parameters about a population, based on the estimates computed from our sample. As such we are interested in statistics about aggregates, rather than in the exact value of single observations. As Little and Rubin (2002) state “It is important to note at the outset that usually sample surveys are conducted with the goal of making inferences about population quantities such as means, correlations and regression coefficients, and the values of individual cases in the data set are not the main interest. Thus, the objective of imputation is not to get the best possible predictions of the missing values, but to replace them by plausible values in order to exploit the information in the recorded variables in the incomplete cases for inference about population parameters.” \n",
    "\n",
    "Cautionary note: many methods to impute missing data. You need to be careful to both use one of the statistically robust methods, and to use a software package that implements it correctly. The theory is very difficult to grasp and there are many permutations possible in implementing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Single imputation\n",
    "\n",
    "= missing values are replaced with a single value derived from the non-missing values for each variable, for example the mean or mode of that variable in your dataset.\n",
    "\n",
    "= most recommended method of imputation, especially in data analysis world. Only method provided by frameworks such as pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insight: you are obviously not changing the mean/mode by using this technique.\n",
    "BUT\n",
    "Variances and covariances will be severely underestimated (Haitovsky, 1968), for two reasons: First, filling in all missing values with the mean will not account for the variation that would most likely be present in reality between those observed values. You are imputing the mean for every missing value, while the real values would probably vary around the mean value. Second, your ultimately increased sample size will result in smaller standard errors, and these will not accurately reflect the uncertainty actually existing in your dataset due to those missing values. \n",
    "\n",
    "In studying single imputation methods, Pigott (2001) concludes that “under no circumstances does mean imputation produce unbiased results…Bias in the estimation of variances and standard errors are compounded when estimating multivariate parameters such as regression coefficients.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about longitudinal studies and the common practice of LOCF and BOCF?\n",
    "\n",
    "They are simply specific cases of single imputation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple imputation\n",
    "\n",
    "To understand what drove the development of multiple imputation, we need to look at a common flaw of all imputation methods reviewed until here. \n",
    "They all lead us to analyze the final dataset as if it were a complete dataset. \n",
    "None of them takes into account the uncertainty present from the missing values in the original dataset. \n",
    "Such analyses invariably lead to overestimated test statistics and underestimated standard errors. \n",
    "\n",
    "All multiple imputation methods therefore have two goals, namely allowing the researcher to \n",
    "1. arrive at unbiased estimates, and \n",
    "2. take into account the uncertainty stemming from the missing data present in the initial dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple imputation methods can be divided between parametric and non parametric ones. \n",
    "\n",
    "parametric: assume data follows an underlying model/distribution. Typically this would be that the data follows a multivariate normal distribution for example.\n",
    "Maximum likelihood, Multiple imputation\n",
    "\n",
    "semi-parametric: relax the assumptions that the data follows an underlying model/distribution\n",
    "Multiple imputation by chained equations MICE (also called fully conditional specification or FCS)\n",
    "\n",
    "non parametric: no assumption about the data is made\n",
    "KNN imputation, hot deck imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum likelihood\n",
    "\n",
    "The EM algorithm\n",
    "The EM algorithm consists of two steps, repeated iteratively until convergence to the ML estimates:\n",
    "\n",
    "1. the expectation step\n",
    "This step is in practice a regression step, where we apply listwise deletion or pairwise deletion to the dataset, and then regress the missing data points for each variable on all the other variables.\n",
    "The algorithm assigns starting values for the unknown parameters: these will be the sample means and covariance. They will be obtain through standard regression means, after applying either of listwise deletion or pairwise deletion.\n",
    "With these starting parameters, it will calculate the coefficients of the regression of the missing values for one variable on all the other variables.\n",
    "Using these regression coefficients, it will then impute the missing values for that variable.\n",
    "This will be done for each variable with missing data.\n",
    "\n",
    "2. After the expectation step is completed, the maximization step will calculate new parameters, means and covariances, using both the imputed data and the nonmissing data.\n",
    "\n",
    "The algorithm then performs again the expectation step, calculating new parameter estimates, regression coefficients, and imputed data.\n",
    "\n",
    "The expectation and maximization steps are repeated until the algorithm converges, which we know because at convergence the estimates will change very little from one iteration to the next.\n",
    "\n",
    "Problems: this method does not output standard error estimates. Mostly suitable for linear regression and linear modelling, for which adaptations and examples of handling missing data through ML have been developed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple imputation\n",
    "\n",
    "The MI algorithm:\n",
    "\n",
    "Step 1: Impute all missing values in your dataset using an appropriate model. There are several robust models, what they all have in common is that they all incorporate a random variation in their estimates for the missing data. This is why they will reflect the uncertainty present in your original dataset, and this is why no two datasets will be the same after imputation, with regards to the originally missing values.\n",
    "Step 2: Perform this imputation m times, hence generating m complete datasets, each slightly different. It is this variability that will adjust the standard errors upwards.\n",
    "Step 3: Apply your analysis to each dataset.\n",
    "Step 4: Pool the results from your m analyses together, therefore reaching one result as if you had dealt with one dataset the entire time. It is at this pooling step that you will incorporate the variability due to imputation. Indeed, the parameter estimate will simply be the mean of the parameter estimates of your m analyses. It is however at the standard error of your final parameter estimates that the magic happens – the latter will capture the estimation uncertainty due to the data originally missing. This pooling technique was developed by Rubin (1987) and is often referred to as “Rubin’s rules.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple imputation by chained equations\n",
    "\n",
    "The main difference between MICE and other multiple imputation techniques is the underlying model assumption. While both ML and MI methods operate under the multivariate normal assumption, MICE does not need to make what is often an unrealistic assumption in large datasets. In MICE, each variable with missing data is modelled as a conditional model that depends on all the other variables in the data. There are three main advantages to this technique:\n",
    "\n",
    "1. each variable can be modelled with its own distribution rather than as part of a joint model for the entire dataset\n",
    "2. as a consequence of 1., MICE is a very versatile MI procedure, allowing us to model binary categorical variables with a logistic regression, multi level categorical variables with polyreg, and continuous variables with predictive mean matching for example. MICE allows us to easily handle in practice different types of variables.\n",
    "3. as a further consequence of 1, MICE has been successfully applied to very large datasets with hundreds of variables (He et al 2009; Stuart et al 2009).\n",
    "\n",
    "The MICE algorithm:\n",
    "\n",
    "Step 1: A starting imputation is performed for every missing value in the dataset. This could be the mean for example.\n",
    "Step 2: For one variable, the values imputed in Step 1 are reset to missing.\n",
    "Step 3: A model is created that regresses the observed values of the variable chosen in Step 2, on the other variables in the dataset. \n",
    "Step 4: The model hence developed in Step 3 is used to predict/impute the missing values for our variable  (the same one as chosen in Step 2 and taken as the dependent variable of the regression model in Step 3).\n",
    "Step 5: These steps are repeated for all variables with missing data until convergence (until the estimates vary very little from one iteration to the next).\n",
    "\n",
    "For a more detailed description of MICE, please see van Buuren (1999, 2011) as well as Azur et al (2011).\n",
    "\n",
    "We only work with MICE in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to the kitchen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# require mice package\n",
    "\n",
    "require(mice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "dataset = read.csv(\"imputation_set_24012018.csv\", header=TRUE, na.strings = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two golden rules to avoid premature ageing:\n",
    "\n",
    "1. Always code your categorical variables as such for R. You need to tell it explicitly, where categorical variables are coded as numbers, that they are categorical. Otherwise it will treat them as continuous, and mess with your imputation model.\n",
    "2. Remove any variables that have all missing values from your dataset before the imputation. It is not possible to impute variables where all data is missing. Duh!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code IDH_TERT and X1p19q_codel as factors so that R/MICE doesnt treat them as continuous\n",
    "# to tell R a variable is categorical, you code it as a factor\n",
    "\n",
    "dataset$IDHTERT<−as.factor(dataset$IDH_TERT)\n",
    "dataset$Tumorgrade<−as.factor(dataset$Tumor_grade)\n",
    "dataset$X1p19qcodel<−as.factor(dataset$X1p19q_codel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run mice imputation with 20 iterations\n",
    "# the number of iterations you want is something you assess subjectively. I tend to go with 20, and so can you.\n",
    "# here we create a new dataset, imp, which is an imputed version of our \"dataset\" with missing values, \n",
    "# and we tell mice to run with 20 iterations (maxit = 20)\n",
    "\n",
    "imp <- mice(dataset, maxit = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# always inspect the methods that mice used to impute each variable, to make sure it imputed the different types of\n",
    "# variables following an appropriate methodology\n",
    "# standard procedure is to impute continuous variables with pmm model, categorical binary with logreg model, and \n",
    "# categorical beyond binary with polyreg model\n",
    "\n",
    "imp$method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we see that mice imputed each variable with a model suitable for its type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now need to inspect convergence\n",
    "# for this, we plot one (or more) parameters, against the iteration\n",
    "\n",
    "plot(imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convergence means that the variance between different imputation sequences is no larger than the variance within each individual sequence.\n",
    "What you want to see if imputations streams that are freely intermingled with each other, without showing any trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add picture of non convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we assessed convergence, it is important to check the imputations.\n",
    "A \"good\" imputed value is one that could have been observed, had it not been missing (van Buuren, 2011)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this, we use two graphs offered by the mice package:\n",
    "\n",
    "1. the kernel density plot, and\n",
    "2. a scatterplot\n",
    "\n",
    "both showing the distribution of observed values, versus that of imputed values.\n",
    "\n",
    "In practice, differences between the observed and imputed values at this level are common. As such, these graphs tend to serve as a guide to the differences that arise in practice between observed and imputed values, as an additional piece of information for investigators. These differences often have their roots in the characteristics of the dataset and those of observed versus missing data (i.e. head circumference measurements were missing for all children under 2 years old in the study, as they had not been measured to begin with. In such a case you will obviously have differences between the distribution of observed values - head circumference of children older than 2 - and imputed values - head circumference of children younger than 2, which was all missing -)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "densityplot(imp.kendall, scales = list(x = list(relation = \"free\")),\n",
    "+    layout = c(5, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## scatterplot (of chl and bmi) for each imputed dataset\n",
    "xyplot(imp, bmi ~ chl | .imp, pch = 20, cex = 1.4)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripplot(imp, pch = 20, cex = 1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now obviously need to export the imputed dataset, for further use in our analysis\n",
    "write.csv(complete(imp), file = \"imputed_dataset_no_censoring_16022018\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Amelia Multiple Imputation\n",
    "\n",
    "# set directory\n",
    "setwd(Dropbox/Healthcare work Patric)\n",
    "\n",
    "# load dataset\n",
    "dataset = read.csv(\"imputation_dataset_no_censoring_24022018\", header = TRUE, na.strings = '')\n",
    "\n",
    "# load Amelia package\n",
    "library(Amelia)\n",
    "\n",
    "# In Amelia coding:\n",
    "noms = categorical variables\n",
    "ords = ordinal variables\n",
    "idvars = nominal variables that should not be imputed\n",
    "\n",
    "# if running Amelia on full dataset, it will throw an error. you need to remove either Tumor_type of Tumor_grade since \n",
    "# removing Tumor_type converges faster, I removed Tumor_grade by adding it to idvars option Since both Tumor_type and \n",
    "# Tumor_grade variables have no missing data, this does not hinder us at all\n",
    "\n",
    "a.out <- amelia(dataset, m=1, noms = c(\"Gender\", \"Tumor_grade\", \"Gene_P53\", \"Gene_Mgmt\", \"Gene_Egfr\", \"Gene_Mdm2\", \"Gene_Cdk4\", \"Gene_P16\", \"Gene_Ihc_Atrx\", \"Gene_Ch10Q\", \"Gene_Ch9P\", \"Tumor_Location\", \"Tumor_Position\", \"Surgery_type\", \"IDH\", \"TERT\", \"X1p19q_codel\"), ords = c(\"IDH_TERT\"), idvars = c(\"RX\", \"CHEM\", \"Tumor_type\"))\n",
    "\n",
    "#get key messages post imputation, including the expected \"Normal EM convergence\"\n",
    "\n",
    "a.out\n",
    "\n",
    "# get full summary of output dataset\n",
    "summary(a.out)\n",
    "\n",
    "# Post imputation diagnostic graphs\n",
    "plot(a.out, which.vars = 4:15)\n",
    "overimpute(a.out, var = \"IDH_TERT\")\n",
    "overimpute(a.out, var = \"IK\")\n",
    "\n",
    "# export dataset. Here a.out contains only one imputed dataset. If it contained 5, this would export each of them \n",
    "# in a csv named file.stem+1, file.stem+2...\n",
    "\n",
    "write.amelia(obj=a.out, file.stem = \"imputed_dataset_no_censoring_24022018_Amelia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN Imputation\n",
    "\n",
    "# code categorical variables as such\n",
    "dataset <- read.csv(\"imputation_dataset_no_censoring_24022018\", header=TRUE, na.strings = \"\")\n",
    "dataset$IDHTERT<−as.factor(dataset$IDH_TERT)\n",
    "dataset$X1p19qcodel<−as.factor(dataset$X1p19q_codel)\n",
    "\n",
    "# impute with KNN method, and using a k=10 parameter\n",
    "imp <- kNN(dataset, k = 10)\n",
    "\n",
    "# export imputed dataset\n",
    "write.csv(imp, \"imputed_dataset_with_censoring_16.02.2018_kNN.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Little's MCAR test\n",
    "\n",
    "https://www.jstor.org/stable/2290157?seq=1#page_scan_tab_contents\n",
    "\n",
    "# we will use the BaylorEdPsych package, which implements Little's test\n",
    "\n",
    "https://cran.r-project.org/web/packages/BaylorEdPsych/BaylorEdPsych.pdf\n",
    "\n",
    "install.packages(\"BaylorEdPsych\")\n",
    "install.packages(\"mvnmle\")\n",
    "\n",
    "require(\"BaylorEdPsych\")\n",
    "require(\"mvnmle\")\n",
    "\n",
    "LittleMCAR(dataset)\n",
    "    \n",
    "# unfortunately R Studio truncates your output. To see your entire output, you need to print your console to an external\n",
    "# file\n",
    "\n",
    "sink(\"output.txt\")\n",
    "print(LittleMCAR(dataset))\n",
    "sink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
